#!/bin/bash

# ì‹¤í–‰ ê¶Œí•œ ìë™ ë¶€ì—¬
chmod +x "$0" 2>/dev/null

# AIStages í™˜ê²½ ë¹ ë¥¸ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
echo "âš¡ AIStages ë¹ ë¥¸ í™˜ê²½ ì²´í¬"
echo "=========================="

# GPU ë©”ëª¨ë¦¬ (ê°€ì¥ ì¤‘ìš”)
echo "ğŸ® GPU ì •ë³´:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
    echo ""
    
    # GPU ë©”ëª¨ë¦¬ MB ë‹¨ìœ„ë¡œ ì¶”ì¶œí•´ì„œ ì „ëµ ì œì•ˆ
    GPU_MEMORY_MB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    GPU_MEMORY_GB=$((GPU_MEMORY_MB / 1024))
    
    echo "ğŸ’¡ ê¶Œì¥ ì‹¤í–‰ ì „ëµ (GPU ë©”ëª¨ë¦¬: ${GPU_MEMORY_GB}GB):"
    if [ $GPU_MEMORY_GB -ge 20 ]; then
        echo "   âœ… ëª¨ë“  ì‹œìŠ¤í…œ ì‹¤í–‰ ê°€ëŠ¥ (V2_1, V2_2, V3)"
        echo "   ğŸ† ì¶”ì²œ: V2_1 ì‹œìŠ¤í…œ (ìµœê³  ì„±ëŠ¥)"
    elif [ $GPU_MEMORY_GB -ge 12 ]; then
        echo "   âœ… V2_2, V3 ì‹œìŠ¤í…œ ê¶Œì¥"
        echo "   ğŸ”¥ ì¶”ì²œ: V3 ê³„ì¸µì  ì‹œìŠ¤í…œ (í˜ì‹ ì )"
    elif [ $GPU_MEMORY_GB -ge 8 ]; then
        echo "   âœ… V2_2 ì‹œìŠ¤í…œ ê¶Œì¥"
        echo "   âš¡ ì¶”ì²œ: íš¨ìœ¨ì  ëª¨ë¸ ìœ„ì£¼"
    else
        echo "   âš ï¸  ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ í•„ìš”"
        echo "   ğŸ’¡ ì¶”ì²œ: ì‘ì€ ëª¨ë¸ + Mixed Precision"
    fi
else
    echo "âŒ GPU ì—†ìŒ - CPU ëª¨ë“œ (ë§¤ìš° ëŠë¦¼)"
fi

echo ""
echo "ğŸ§® ë©”ëª¨ë¦¬:"
free -h | grep Mem

echo ""
echo "ğŸ’¾ ë””ìŠ¤í¬ ê³µê°„:"
df -h . | tail -1

echo ""
echo "ğŸ Python & PyTorch:"
python -c "
import torch
print(f'Python: {torch.version.__version__ if hasattr(torch.version, \"__version__\") else \"Unknown\"}')
print(f'PyTorch: {torch.__version__}')
print(f'CUDA: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
" 2>/dev/null || echo "PyTorch ì„¤ì¹˜ í™•ì¸ í•„ìš”"

echo ""
echo "ğŸš€ ë¹ ë¥¸ ì‹¤í–‰ ëª…ë ¹ì–´:"
echo "   ì „ì²´ ì‹œìŠ¤í…œ ì²´í¬: ./check_aistages_system.sh"
echo "   ML í˜¸í™˜ì„± ì²´í¬: ./check_ml_compatibility.sh"
echo "   V2_2 ì‹¤í—˜ ì‹œì‘: ./run_v2_2_only.sh"
echo ""
