import os
import wandb
import torch
import numpy as np
from tqdm import tqdm
import copy
import time
from types import SimpleNamespace
from sklearn.metrics import f1_score

import sys
# cv-classification í™˜ê²½ì— ë§ê²Œ ë™ì ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from codes.gemini_augmentation_v3 import get_augmentation, apply_mixup_cutmix, mixup_criterion

class EarlyStopping:
    def __init__(self, patience=5, min_delta=1e-6, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_model_state_dict = None
        self.best_loss = None
        self.counter = 0
        self.status = ""

    def __call__(self, model, val_loss):
        if self.best_loss is None:
            #í˜„ì¬ì˜ ëª¨ë¸ë¡œ self.best_loss, self.best_model_state_dict ì—…ë°ì´íŠ¸
            self.best_loss = val_loss
            self.best_model_state_dict = copy.deepcopy(model.state_dict())
        elif val_loss < self.best_loss - self.min_delta:
			#val_lossê°€ best_lossë³´ë‹¤ ì¢‹ì„ ë•Œ > self.best_lossì™€ self.best_model ì—…ë°ì´íŠ¸
            self.best_model_state_dict = copy.deepcopy(model.state_dict())
            self.best_loss = val_loss
            self.counter = 0
            self.status = f"Improvement found, counter reset to {self.counter}"
        else:
			#val_lossê°€ ë” ì•ˆ ì¢‹ì„ ë•Œ > patience ì¦ê°€í•˜ê³  early stop ì—¬ë¶€ í™•ì¸
            self.counter += 1
            self.status = f"No improvement in the last {self.counter} epochs"
            if self.counter >= self.patience:
                self.status = f"Early stopping triggered after {self.counter} epochs."
                if self.restore_best_weights and self.best_model_state_dict is not None:
                    model.load_state_dict(self.best_model_state_dict)
                return True # ealy stopped
        return False # end with no early stop
    
    def restore_best(self, model):
        if self.best_loss is not None and self.best_model_state_dict is not None:
            print(f"Restore model_state_dict of which best_loss: {self.best_loss:.6f}")
            model.load_state_dict(self.best_model_state_dict)
            return True
        return False
	
class TrainModule():
	def __init__(self, model: torch.nn.Module, criterion, optimizer, scheduler, train_loader, valid_loader, cfg: SimpleNamespace, verbose:int =50, run=None):
		'''
		model, criterion, scheduler, train_loader, valid_loader ë¯¸ë¦¬ ì •ì˜í•´ì„œ ì „ë‹¬
		cfg : es_patience, epochs ë“±ì— ëŒ€í•œ hyperparametersë¥¼ namespace ê°ì²´ë¡œ ì…ë ¥
		'''
		required_attrs = ['scheduler_name','patience', 'epochs']
		for attr in required_attrs:
			assert hasattr(cfg, attr), f"AttributeError: There's no '{attr}' attribute in cfg."
		assert verbose > 0 and verbose < cfg.epochs, f"Logging frequency({verbose}) MUST BE smaller than EPOCHS({cfg.epochs}) and positive value."
		
		self.model = model
		self.criterion = criterion
		self.optimizer = optimizer
		self.scheduler = scheduler
		self.train_loader = train_loader
		self.valid_loader = valid_loader
		self.cfg = cfg
		if getattr(cfg, "device", False):
			self.model.to(self.cfg.device)
		else:
			self.cfg.device = 'cpu'
		self.es = EarlyStopping(patience=self.cfg.patience)
		### list for plot
		self.train_losses_for_plot, self.val_losses_for_plot = [], []
		self.train_acc_for_plot, self.val_acc_for_plot = [], [] # classification
		self.train_f1_for_plot, self.val_f1_for_plot = [], [] # classification
		# logging frequency
		self.verbose = verbose
		# wandb run object
		self.run = run
		
		# ğŸ”§ í”Œë«í¼ë³„ Mixed Precision ë° ìºì‹œ ê´€ë¦¬ ì„¤ì •
		self.device_str = str(self.cfg.device)
		self.is_cuda = self.device_str.startswith('cuda')
		self.is_mps = self.device_str.startswith('mps')
		self.is_cpu = self.device_str == 'cpu'
		
		# Mixed Precision ì„¤ì •: CUDAì—ì„œë§Œ ì™„ì „íˆ ì§€ì›ë¨
		self.use_mixed_precision = self.cfg.mixed_precision and self.is_cuda
		if self.cfg.mixed_precision and not self.is_cuda:
			print(f"âš ï¸ Mixed Precisionì€ CUDAì—ì„œë§Œ ì™„ì „íˆ ì§€ì›ë©ë‹ˆë‹¤. í˜„ì¬ device: {self.device_str}")
			print("âš ï¸ Mixed Precisionì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
			
		# autocastìš© device_type ì„¤ì •
		self.autocast_device_type = 'cuda' if self.is_cuda else None
		
		# GradScalerëŠ” CUDAì—ì„œë§Œ ì‚¬ìš©
		self.scaler = torch.amp.GradScaler(enabled=self.use_mixed_precision)
		
		# ğŸ”¥ Phase 1: MixUp/CutMix ì§€ì› í™•ì¸
		self.use_mixup_cutmix = hasattr(cfg, 'mixup_cutmix') and getattr(cfg.mixup_cutmix, 'prob', 0) > 0
		if self.use_mixup_cutmix:
			print(f"ğŸ”¥ MixUp/CutMix enabled with prob={cfg.mixup_cutmix['prob']}")
		
		print(f"ğŸ”§ Device: {self.device_str}")
		print(f"ğŸ”§ Mixed Precision: {'Enabled' if self.use_mixed_precision else 'Disabled'}")
		print(f"ğŸ”§ AutoCast Device Type: {self.autocast_device_type}")
		print(f"ğŸ”¥ MixUp/CutMix: {'Enabled' if self.use_mixup_cutmix else 'Disabled'}")
		
	def _clear_cache(self):
		"""í”Œë«í¼ë³„ GPU/MPS ìºì‹œ ë¹„ìš°ê¸°"""
		if self.is_cuda:
			torch.cuda.empty_cache()
		elif self.is_mps:
			# MPSëŠ” PyTorch 2.0+ì—ì„œ torch.mps.empty_cache() ì§€ì›
			try:
				torch.mps.empty_cache()
			except AttributeError:
				# torch.mps.empty_cache()ê°€ ì—†ëŠ” ê²½ìš° (ì´ì „ ë²„ì „)
				pass
		# CPUëŠ” ìºì‹œ ë¹„ìš¸ í•„ìš” ì—†ìŒ
		
	def training_step(self):
		# set train mode
		self.model.train()
		running_loss = 0.0
		correct = 0 # classification
		total = 0
		all_preds = []
		all_targets = []
		
		for train_x, train_y in self.train_loader: # batch training
			train_x, train_y = train_x.to(self.cfg.device), train_y.to(self.cfg.device)
			
			# ğŸ”¥ Phase 1: MixUp/CutMix ì ìš©
			if self.use_mixup_cutmix:
				mixed_x, y_a, y_b, lam, use_mixup_cutmix = apply_mixup_cutmix(
					train_x, train_y, self.cfg, self.cfg.device
				)
			else:
				mixed_x, y_a, y_b, lam, use_mixup_cutmix = train_x, train_y, train_y, 1.0, False
			
			self.optimizer.zero_grad() # ì´ì „ gradient ì´ˆê¸°í™”

			# ğŸ”§ í”Œë«í¼ë³„ Mixed Precision ì²˜ë¦¬
			if self.use_mixed_precision and self.autocast_device_type:
				# CUDAì—ì„œë§Œ Mixed Precision ì‚¬ìš©
				with torch.amp.autocast(device_type=self.autocast_device_type, enabled=True):
					outputs = self.model(mixed_x)
					
					# ğŸ”¥ Phase 1: MixUp/CutMix ì†ì‹¤ ê³„ì‚°
					if use_mixup_cutmix:
						loss = mixup_criterion(self.criterion, outputs, y_a, y_b, lam)
					else:
						loss = self.criterion(outputs, train_y)
						
				self.scaler.scale(loss).backward()
				self.scaler.step(self.optimizer)
				self.scaler.update()
			else:
				# MPS, CPU ë˜ëŠ” Mixed Precision ë¹„í™œì„±í™”ì‹œ ì¼ë°˜ ì²˜ë¦¬
				outputs = self.model(mixed_x)
				
				# ğŸ”¥ Phase 1: MixUp/CutMix ì†ì‹¤ ê³„ì‚°
				if use_mixup_cutmix:
					loss = mixup_criterion(self.criterion, outputs, y_a, y_b, lam)
				else:
					loss = self.criterion(outputs, train_y)
					
				loss.backward()
				self.optimizer.step()

			if self.cfg.scheduler_name == "OneCycleLR":
				self.scheduler.step()
			
			running_loss += loss.item() * train_y.size(0) # train_loss 
			
			# ğŸ”¥ Phase 1: MixUp/CutMix ì‚¬ìš©ì‹œ ì •í™•ë„ ê³„ì‚° ë°©ì‹ ì¡°ì •
			_, predicted = torch.max(outputs, 1) # ê°€ì¥ í™•ë¥  ë†’ì€ í´ë˜ìŠ¤ ì˜ˆì¸¡
			if use_mixup_cutmix:
				# MixUp/CutMix ì‚¬ìš©ì‹œ ê·¼ì‚¬ ì •í™•ë„ ê³„ì‚°
				correct += (lam * (predicted == y_a).sum().item() + 
						   (1 - lam) * (predicted == y_b).sum().item())
			else:
				correct += (predicted == train_y).sum().item()
			
			total += train_y.size(0) 

			# F1 scoreë¥¼ ìœ„í•´ ì›ë˜ ë¼ë²¨ ì‚¬ìš©
			all_preds.extend(predicted.cpu().numpy())
			all_targets.extend(train_y.cpu().numpy())

			# ğŸ”§ í”Œë«í¼ë³„ ë©”ëª¨ë¦¬ ìºì‹œ ë¹„ìš°ê¸°
			del train_x, train_y, outputs, loss
			if use_mixup_cutmix:
				del mixed_x, y_a, y_b
			self._clear_cache()
			
		epoch_loss = running_loss / total # average loss of 1 epoch
		epoch_acc = 100 * correct / total # classification
		epoch_f1 = f1_score(all_targets, all_preds, average='macro') # classification
		return epoch_loss, epoch_acc, epoch_f1  # classification		
	
	def validation_step(self):
		if self.cfg.tta_dropout: 
			# inference ì‹œì—ë„ dropoutì„ ìœ ì§€í•˜ì—¬ ë§ˆì¹˜ ì•™ìƒë¸”í•˜ëŠ” ê²ƒ ê°™ì€ íš¨ê³¼ë¥¼ ì¤€ë‹¤.
			self.model.train()
		else:
			self.model.eval()  # í‰ê°€ ëª¨ë“œ
		self.model.eval()  # í‰ê°€ ëª¨ë“œ
		val_loss = 0
		correct = 0 # classification
		total = 0
		all_preds = []
		all_targets = []
		
		with torch.no_grad():  # gradient ê³„ì‚° ë¹„í™œì„±í™”
			for val_x, val_y in self.valid_loader: # batch training
				val_x, val_y = val_x.to(self.cfg.device), val_y.to(self.cfg.device)
				
				# ğŸ”§ í”Œë«í¼ë³„ Mixed Precision ì²˜ë¦¬
				if self.use_mixed_precision and self.autocast_device_type:
					# CUDAì—ì„œë§Œ Mixed Precision ì‚¬ìš©
					with torch.amp.autocast(device_type=self.autocast_device_type, enabled=True):
						outputs = self.model(val_x)
						loss = self.criterion(outputs, val_y)
				else:
					# MPS, CPU ë˜ëŠ” Mixed Precision ë¹„í™œì„±í™”ì‹œ ì¼ë°˜ ì²˜ë¦¬
					outputs = self.model(val_x)
					loss = self.criterion(outputs, val_y)
								
				val_loss += loss.item() * val_y.size(0)
				_, predicted = torch.max(outputs, 1) # classification
				correct += (predicted == val_y).sum().item() # classification
				total += val_y.size(0)

				all_preds.extend(predicted.cpu().numpy())
				all_targets.extend(val_y.cpu().numpy())

				# ğŸ”§ í”Œë«í¼ë³„ ë©”ëª¨ë¦¬ ìºì‹œ ë¹„ìš°ê¸°
				del val_x, val_y, outputs, loss
				self._clear_cache()
		
		epoch_loss = val_loss / total # average loss of 1 epoch
		epoch_acc = 100 * correct / total # classification
		epoch_f1 = f1_score(all_targets, all_preds, average='macro') # classification
		return epoch_loss, epoch_acc, epoch_f1 # classification
	
	def update_transform(self, epoch):
		train_transforms, _, _, _ = get_augmentation(self.cfg, epoch)
		if self.cfg.online_augmentation:
			self.train_loader.dataset.transform = train_transforms[0]
		else:
			# ConcatDatasetì˜ ê²½ìš°, ê° sub-datasetì˜ transformì„ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.
			for i, dataset in enumerate(self.train_loader.dataset.datasets):
				dataset.transform = train_transforms[i]

	def training_loop(self):
		# try:
		# reset loss list for plots
		self.train_losses_for_plot, self.val_losses_for_plot = [], []
		self.train_acc_for_plot, self.val_acc_for_plot = [], []
		epoch_counter = 0
		epoch_timer = []
		done = False
		
		pbar = tqdm(total=self.cfg.epochs)
		while not done and epoch_counter<self.cfg.epochs:
			self.update_transform(epoch_counter) # epochì— ë”°ë¼ ì¦ê°• ê¸°ë²•ì„ ë°”ê¾¼ë‹¤.
			st = time.time()
			epoch_counter += 1
			
			# train
			# train_loss = self.training_step() # regression
			train_loss, train_acc, train_f1 = self.training_step() # classification
			
			self.train_losses_for_plot.append(train_loss)
			self.train_acc_for_plot.append(train_acc) # classification
			self.train_f1_for_plot.append(train_f1) # classification

			# schedulerì˜ ì¢…ë¥˜ì— ë”°ë¼ val_lossë¥¼ ì „ë‹¬í•˜ê±°ë‚˜ ê·¸ëƒ¥ step() í˜¸ì¶œ.
			if self.cfg.scheduler_name == "OneCycleLR":
				pass
			elif self.cfg.scheduler_name == "ReduceLROnPlateau":
				self.scheduler.step(val_loss)
			else:
				self.scheduler.step()

			# validation
			# val_loss = self.validation_step() # regression
			val_loss, val_acc, val_f1 = self.validation_step()  # classification
			self.val_losses_for_plot.append(val_loss)
			self.val_acc_for_plot.append(val_acc) # classification
			self.val_f1_for_plot.append(val_f1) # classification

			epoch_timer.append(time.time() - st)
			pbar.update(1)
			
			if self.run is not None:
				# print('wandb logging...')
				epoch_log = {
					'train_loss': train_loss,
					'train_accuracy': train_acc,
					'train_f1': train_f1,
					'val_loss': val_loss,
					'val_accuracy': val_acc,
					'val_f1': val_f1, 
					'learning_rate': self.optimizer.param_groups[0]['lr'],
				}

				# logging weights & gradients
				all_grads = []
				all_weights = []
				for param in self.model.parameters():
					if param.data is not None:
						all_weights.append(param.data.cpu().view(-1))
					if param.grad is not None:
						all_grads.append(param.grad.cpu().view(-1))
				if all_grads:
					epoch_log['weight/all'] = wandb.Histogram(torch.cat(all_weights))
				if all_weights:
					epoch_log['weights/all'] = wandb.Histogram(torch.cat(all_weights))
				
				self.run.log(epoch_log, step=epoch_counter) # wandb logging
			if epoch_counter == 1 or epoch_counter % self.verbose == 0:
				# self.verbose epochë§ˆë‹¤ logging
				mean_time_spent = np.mean(epoch_timer)
				epoch_timer = [] # reset timer list
				# print(f"Epoch {epoch_counter}/{self.cfg.epochs} [Time: {mean_time_spent:.2f}s], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.8f}")
				print(f"Epoch {epoch_counter}/{self.cfg.epochs} [Time: {mean_time_spent:.2f}s], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.8f}\n Train ACC: {train_acc:.2f}%, Validation ACC: {val_acc:.2f}%\n Train F1: {train_f1:.4f}, Validation F1: {val_f1:.4f}") # classification

			if self.es(self.model, val_loss):
				# early stopped ëœ ê²½ìš° if ë¬¸ ì•ˆìœ¼ë¡œ ë“¤ì–´ì˜¨ë‹¤.
				done = True
		# except Exception as e:
		# 	print(e)
		# 	return False # training loop failed
		return True # training loop succeed
		
	def plot_loss(self, show:bool=False, savewandb:bool=True, savedir:str=None):
		"""loss, accuracy, f1-scoreì— ëŒ€í•œ ê·¸ë˜í”„ ì‹œê°í™” í•¨ìˆ˜

		:param bool show: plt.show()ë¥¼ ì‹¤í–‰í•  ê±´ì§€, defaults to False
		:param bool savewandb: wandb loggingì— plotì„ ì‹œê°í™”í•˜ì—¬ ì €ì¥í•  ê±´ì§€, defaults to True
		:param str savedir: plotì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •, Noneì´ë©´ ì €ì¥ ì•ˆ í•¨, defaults to None
		:return _type_: None
		"""
		import matplotlib.pyplot as plt
		fig, ax = plt.subplots(figsize=(6, 4))
		plt.plot(range(len(self.train_losses_for_plot)),self.train_losses_for_plot,color='blue',label='train_loss')
		plt.plot(range(len(self.val_losses_for_plot)),self.val_losses_for_plot,color='red',label='val_loss')
		plt.axhline(y=1e-3, color='red', linestyle='--', label='(Overfit)')
		plt.legend()
		plt.xlabel("Epoch")
		plt.ylabel("Loss")
		plt.title("Train/Validation Loss plot")
		if savedir is not None:
			if os.path.exists(savedir):
				os.makedirs(savedir, exist_ok=True)
			savepath = os.path.join(savedir, "loss_plot.png")
			plt.savefig(savepath)
			print(f"âš™ï¸loss plot saved in {savepath}")
		if show:
			plt.show()
		if savewandb and self.run is not None:
			self.run.log({'loss_plot': wandb.Image(fig)}) # wandb
		plt.clf()
		
		# classification
		fig, ax = plt.subplots(figsize=(6, 4))
		plt.plot(range(len(self.train_acc_for_plot)),self.train_acc_for_plot,color='blue',label='train_acc')
		plt.plot(range(len(self.val_acc_for_plot)),self.val_acc_for_plot,color='red',label='val_acc')
		plt.axhline(y=99.0, color='red', linestyle='--', label='(99%)')
		plt.legend()
		plt.xlabel("Epoch")
		plt.ylabel("Accuracy(%)")
		plt.title("Train/Validation Accuracy Plot")
		plt.grid()
		if savedir is not None:
			savepath = os.path.join(savedir, "accuracy_plot.png")
			plt.savefig(savepath)
			print(f"âš™ï¸accuracy plot saved in {savepath}")
		if show:
			plt.show()
		if savewandb and self.run is not None:
			self.run.log({'accuracy_plot': wandb.Image(fig)}) # wandb
		plt.clf()

		# classification
		fig, ax = plt.subplots(figsize=(6, 4))
		plt.plot(range(len(self.train_f1_for_plot)),self.train_f1_for_plot,color='blue',label='train_f1')
		plt.plot(range(len(self.val_f1_for_plot)),self.val_f1_for_plot,color='red',label='val_f1')
		plt.axhline(y=0.99, color='red', linestyle='--', label='(0.99)')
		plt.legend()
		plt.xlabel("Epoch")
		plt.ylabel("F1-score")
		plt.title("Train/Validation F1-score Plot")
		plt.grid()
		if savedir is not None:
			savepath = os.path.join(savedir, "f1_plot.png")
			plt.savefig(savepath)
			print(f"âš™ï¸f1 plot saved in {savepath}")
		if show:
			plt.show()
		if savewandb and self.run is not None:
			self.run.log({'f1_plot': wandb.Image(fig)}) # wandb
		plt.clf()
		return None
		
	def save_experiments(self, savepath=None):
		""""""
		save_dict = {
			'model_state_dict': self.model.state_dict(),
			'optimizer_state_dict': self.optimizer.state_dict(),
			'scheduler_state_dict': self.scheduler.state_dict(),
			'cfg': vars(self.cfg) # ë‚˜ì¤‘ì— ë¡œë“œí•´ì„œ CFG = SimpleNamespace(**cfg)ë¡œ ë³µì›
		}
		if savepath is not None:
			dirpath = os.path.dirname(savepath)
			if os.path.exists(dirpath):
				os.makedirs(dirpath, exist_ok=True)
			torch.save(save_dict, f=savepath)
			return True
		return False
