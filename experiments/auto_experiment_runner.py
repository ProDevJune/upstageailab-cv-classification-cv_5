#!/usr/bin/env python3
"""
ìë™ ì‹¤í—˜ ì‹¤í–‰ê¸°
ìƒì„±ëœ ì‹¤í—˜ íë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import subprocess
import time
import gc
import psutil
import torch
import yaml
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import traceback
from tqdm import tqdm

# ì‹¤í—˜ ê²©ë¦¬ ìœ í‹¸ë¦¬í‹° ì¶”ê°€
from isolation_utils import (
    setup_experiment_isolation,
    cleanup_experiment_temp_dirs,
    validate_system_state,
    deep_cleanup,
    wait_for_system_stability,
    ExperimentIsolationContext
)


class GPUMemoryManager:
    """GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def clear_gpu_memory():
        """GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            gc.collect()
    
    @staticmethod
    def get_gpu_memory_info():
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
        if not torch.cuda.is_available():
            return "GPU not available"
        
        device = torch.cuda.current_device()
        total = torch.cuda.get_device_properties(device).total_memory / 1024**3
        allocated = torch.cuda.memory_allocated(device) / 1024**3
        cached = torch.cuda.memory_reserved(device) / 1024**3
        
        return f"GPU Memory - Total: {total:.1f}GB, Allocated: {allocated:.1f}GB, Cached: {cached:.1f}GB"


class ExperimentRunner:
    """ìë™ ì‹¤í—˜ ì‹¤í–‰ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, queue_file: str, resume: bool = False):
        self.queue_file = queue_file
        self.base_dir = Path(queue_file).parent.parent
        self.logs_dir = self.base_dir / "experiments" / "logs"
        self.submissions_dir = self.base_dir / "experiments" / "submissions"
        self.resume = resume
        
        # ì‹¤í—˜ í ë¡œë“œ
        self.queue_data = self._load_queue()
        self.experiments = self.queue_data['experiments']
        
        # ì§„í–‰ ìƒí™© ì¶”ì 
        self.completed_experiments = []
        self.failed_experiments = []
        self.current_experiment = None
        self.start_time = None
        
    def _load_queue(self) -> Dict:
        """ì‹¤í—˜ í íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.queue_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise Exception(f"ì‹¤í—˜ í íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _save_queue(self):
        """ì‹¤í—˜ í ìƒíƒœ ì €ì¥"""
        try:
            with open(self.queue_file, 'w', encoding='utf-8') as f:
                json.dump(self.queue_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸  í ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _check_resume_status(self) -> int:
        """ì¤‘ë‹¨ëœ ì‹¤í—˜ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹œì‘ ì§€ì  ë°˜í™˜"""
        if not self.resume:
            return 0
        
        completed_count = 0
        for i, exp in enumerate(self.experiments):
            if exp['status'] == 'completed':
                completed_count += 1
            elif exp['status'] == 'running':
                # ì‹¤í–‰ ì¤‘ì´ë˜ ì‹¤í—˜ì„ pendingìœ¼ë¡œ ë˜ëŒë¦¼
                exp['status'] = 'pending'
                self._save_queue()
                break
            elif exp['status'] == 'pending':
                break
        
        if completed_count > 0:
            print(f"ğŸ”„ Resume ëª¨ë“œ: {completed_count}ê°œ ì‹¤í—˜ ì™„ë£Œë¨, {completed_count + 1}ë²ˆì§¸ë¶€í„° ì‹œì‘")
        
        return completed_count
    
    def _update_experiment_status(self, experiment_id: str, status: str, 
                                additional_info: Dict = None):
        """ì‹¤í—˜ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        for exp in self.experiments:
            if exp['experiment_id'] == experiment_id:
                exp['status'] = status
                exp['last_updated'] = datetime.now().isoformat()
                
                if additional_info:
                    exp.update(additional_info)
                break
        
        self._save_queue()
    
    def _generate_memo_suggestion(self, model_name: str, technique_name: str, 
                                config_path: str) -> Dict:
        """ì œì¶œìš© ë©”ëª¨ ìë™ ìƒì„± (OCR ì§€ì›)"""
        
        # ëª¨ë¸ëª… ì¶•ì•½
        model_abbrev = {
            'swin_transformer': 'SwinB384',
            'efficientnet_b4': 'EffNetB4',
            'convnext_base': 'ConvNeXtB',
            'maxvit_base': 'MaxViTB384'
        }.get(model_name, model_name[:10])
        
        # ê¸°ë²• ì¶•ì•½
        technique_abbrev = {
            'baseline': 'Base',
            'focal_loss': 'Focal',
            'mixup_cutmix': 'Mix50%',
            'focal_mixup': 'Focal+Mix50%',
            'label_smooth': 'LabelSmooth',
            'label_mixup': 'LabelSmooth+Mix50%'
        }.get(technique_name, technique_name[:10])
        
        # ê¸°ë³¸ ë©”ëª¨ ìƒì„±
        base_memo = f"{model_abbrev}+{technique_abbrev}"
        
        # ì„¤ì • íŒŒì¼ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        ocr_enabled = False
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # OCR ì •ë³´ í™•ì¸
            if 'ocr' in config and config['ocr'].get('enabled', False):
                ocr_enabled = True
                ocr_model = config['ocr'].get('ocr_model', 'OCR')
                if ocr_model == 'OCR':
                    base_memo += "+OCR"
                else:
                    base_memo += f"+{ocr_model}"
            
            # TTA ì •ë³´ ì¶”ê°€
            if config.get('test_TTA', False):
                base_memo += "+TTA"
            
            # Focal Loss íŒŒë¼ë¯¸í„° ì¶”ê°€
            if 'focal' in technique_name and 'focal_loss' in config:
                alpha = config['focal_loss'].get('alpha', 1.0)
                gamma = config['focal_loss'].get('gamma', 2.0)
                if alpha != 1.0 or gamma != 2.0:
                    base_memo += f"(Î±{alpha},Î³{gamma})"
                    
        except Exception:
            pass
        
        # 50ì ì œí•œ í™•ì¸
        if len(base_memo) > 50:
            # ì§€ëŠ¥ì  ì¶•ì•½
            if "+TTA" in base_memo:
                base_memo = base_memo.replace("+TTA", "")
            if len(base_memo) > 50:
                base_memo = base_memo[:47] + "..."
        
        # ëŒ€ì•ˆ ë©”ëª¨ë“¤ ìƒì„±
        ocr_suffix = " OCR" if ocr_enabled else ""
        alternatives = [
            f"{model_abbrev} {technique_abbrev}{ocr_suffix}",
            f"{model_name.split('_')[0]} + {technique_name}{ocr_suffix}",
            f"Auto: {model_abbrev}+{technique_abbrev}{ocr_suffix}"
        ]
        
        alternatives = [alt for alt in alternatives if len(alt) <= 50 and alt != base_memo]
        
        return {
            'auto_generated': base_memo,
            'character_count': len(base_memo),
            'alternatives': alternatives[:3]
        }
    
    def _create_experiment_result_log(self, experiment: Dict, 
                                    success: bool, error_msg: str = None, 
                                    actual_results: Dict = None) -> str:
        """ì‹¤í—˜ ê²°ê³¼ JSON ë¡œê·¸ ìƒì„± (ì‹¤ì œ ê²°ê³¼ íŒŒì‹± ì§€ì›)"""
        experiment_id = experiment['experiment_id']
        log_file = self.logs_dir / f"{experiment_id}.json"
        
        # ê¸°ë³¸ ê²°ê³¼ êµ¬ì¡°
        result_data = {
            "experiment_id": experiment_id,
            "timestamp": datetime.now().isoformat(),
            "model": experiment['model_name'],
            "technique": experiment['technique_name'],
            "config_path": experiment['config_path'],
            "description": experiment['description'],
            "success": success
        }
        
        if success:
            # ì„±ê³µì ì¸ ì‹¤í—˜ ê²°ê³¼ ì²˜ë¦¬
            try:
                # ì‹¤ì œ ê²°ê³¼ê°€ ì „ë‹¬ëœ ê²½ìš° ì‚¬ìš©, ì—†ìœ¼ë©´ íŒŒì‹± ì‹œë„
                if actual_results:
                    local_results = actual_results
                else:
                    local_results = self._parse_experiment_results(experiment_id)
                
                result_data.update({
                    "local_results": local_results,
                    "submission": {
                        "csv_path": str(self.submissions_dir / f"{experiment_id}_submission.csv"),
                        "submission_ready": True,
                        "file_size_mb": 2.5,  # placeholder
                        "created_at": datetime.now().isoformat()
                    },
                    "memo_suggestion": self._generate_memo_suggestion(
                        experiment['model_name'], 
                        experiment['technique_name'],
                        experiment['config_path']
                    ),
                    "server_evaluation": {
                        "submitted": False,
                        "submission_date": None,
                        "server_score": None,
                        "server_rank": None,
                        "notes": "",
                        "performance_gap": None
                    }
                })
            except Exception as e:
                print(f"âš ï¸  ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                result_data["parsing_error"] = str(e)
        else:
            # ì‹¤íŒ¨í•œ ì‹¤í—˜ ì²˜ë¦¬
            result_data.update({
                "error_message": error_msg,
                "local_results": None,
                "submission": {
                    "csv_path": None,
                    "submission_ready": False,
                    "file_size_mb": 0,
                    "created_at": None
                },
                "memo_suggestion": None,
                "server_evaluation": None
            })
        
        # JSON íŒŒì¼ ì €ì¥
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        return str(log_file)
    
    def _parse_experiment_results(self, experiment_id: str) -> Dict:
        """ì‹¤í—˜ ê²°ê³¼ íŒŒì‹± (ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì‹¤ì œ ê²°ê³¼ ì¶”ì¶œ)"""
        try:
            # 1. W&B ë¡œê·¸ì—ì„œ ê²°ê³¼ ì¶”ì¶œ ì‹œë„
            wandb_results = self._parse_wandb_results(experiment_id)
            if wandb_results:
                return wandb_results
            
            # 2. ì œì¶œ íŒŒì¼ CSVì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            submission_results = self._parse_submission_metadata(experiment_id)
            if submission_results:
                return submission_results
            
            # 3. ë¡œê·¸ íŒŒì¼ì—ì„œ ì¶”ì¶œ
            log_results = self._parse_console_logs(experiment_id)
            if log_results:
                return log_results
            
            # 4. ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ
            checkpoint_results = self._parse_checkpoint_metadata(experiment_id)
            if checkpoint_results:
                return checkpoint_results
            
            print(f"âš ï¸ ê²½ê³ : {experiment_id}ì˜ ì‹¤ì œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©.")
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        # ê¸°ë³¸ê°’ ë°˜í™˜ (íŒŒì‹± ì‹¤íŒ¨ ì‹œ)
        return {
            "validation_f1": None,
            "validation_acc": None,
            "training_time_minutes": None,
            "total_epochs": None,
            "early_stopped": None,
            "parsing_failed": True,
            "note": "Results parsing failed - may need manual verification"
        }
    
    def _parse_wandb_results(self, experiment_id: str) -> Dict:
        """ì›¹ë“œì•¤ë¹„ ë¡œê·¸ì—ì„œ ê²°ê³¼ ì¶”ì¶œ"""
        try:
            import wandb
            api = wandb.Api()
            
            # ì‹¤í—˜ IDë¡œ W&B ëŸ° ì°¾ê¸°
            runs = api.runs(f"auto-exp-ocr-{experiment_id}", 
                          filters={"state": "finished"})
            
            if runs:
                run = runs[0]  # ê°€ì¥ ìµœê·¼ ëŸ°
                
                # ë©”íŠ¸ë¦­ ì¶”ì¶œ
                summary = run.summary
                history = run.history()
                
                validation_f1 = summary.get('val_f1', summary.get('validation_f1', None))
                validation_acc = summary.get('val_acc', summary.get('validation_acc', None))
                
                # í•™ìŠµ ì‹œê°„ ê³„ì‚°
                if run.created_at and run.updated_at:
                    training_time = (run.updated_at - run.created_at).total_seconds() / 60
                else:
                    training_time = None
                
                return {
                    "validation_f1": float(validation_f1) if validation_f1 else None,
                    "validation_acc": float(validation_acc) if validation_acc else None,
                    "training_time_minutes": int(training_time) if training_time else None,
                    "total_epochs": summary.get('epoch', None),
                    "early_stopped": summary.get('early_stopped', None),
                    "wandb_run_id": run.id,
                    "source": "wandb"
                }
                
        except Exception as e:
            print(f"ğŸ” W&B íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_submission_metadata(self, experiment_id: str) -> Dict:
        """ì œì¶œ íŒŒì¼ CSV ë©”íƒ€ë°ì´í„°ì—ì„œ ê²°ê³¼ ì¶”ì¶œ"""
        try:
            submission_file = self.submissions_dir / f"{experiment_id}_submission.csv"
            if submission_file.exists():
                # CSV íŒŒì¼ ë©”íƒ€ë°ì´í„° ë˜ëŠ” ë™ë°˜ JSON íŒŒì¼ í™•ì¸
                metadata_file = submission_file.with_suffix('.json')
                if metadata_file.exists():
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                        return {
                            "validation_f1": metadata.get('validation_f1'),
                            "validation_acc": metadata.get('validation_acc'),
                            "training_time_minutes": metadata.get('training_time_minutes'),
                            "total_epochs": metadata.get('total_epochs'),
                            "early_stopped": metadata.get('early_stopped'),
                            "source": "submission_metadata"
                        }
        except Exception as e:
            print(f"ğŸ” ì œì¶œ ë©”íƒ€ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_console_logs(self, experiment_id: str) -> Dict:
        """ì½˜ì†” ë¡œê·¸ì—ì„œ ê²°ê³¼ ì¶”ì¶œ"""
        try:
            # ì²´í¬í¬ì¸íŠ¸ë‚˜ ë¡œê·¸ íŒŒì¼ì—ì„œ ìµœì¢… ê²°ê³¼ ì°¾ê¸°
            possible_log_files = [
                self.base_dir / "models" / f"{experiment_id}_best.pth",
                self.base_dir / "logs" / f"{experiment_id}.log",
                self.base_dir / f"{experiment_id}_results.txt"
            ]
            
            for log_file in possible_log_files:
                if log_file.exists():
                    with open(log_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ê²°ê³¼ ì¶”ì¶œ
                        import re
                        
                        f1_match = re.search(r'(?:val_f1|validation_f1|f1[_\s]*score)[:\s]*([0-9.]+)', content, re.IGNORECASE)
                        acc_match = re.search(r'(?:val_acc|validation_acc|accuracy)[:\s]*([0-9.]+)', content, re.IGNORECASE)
                        epoch_match = re.search(r'(?:epoch|epochs)[:\s]*([0-9]+)', content, re.IGNORECASE)
                        
                        if f1_match or acc_match:
                            return {
                                "validation_f1": float(f1_match.group(1)) if f1_match else None,
                                "validation_acc": float(acc_match.group(1)) if acc_match else None,
                                "training_time_minutes": None,
                                "total_epochs": int(epoch_match.group(1)) if epoch_match else None,
                                "early_stopped": "early" in content.lower(),
                                "source": f"log_file_{log_file.name}"
                            }
                            
        except Exception as e:
            print(f"ğŸ” ë¡œê·¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_checkpoint_metadata(self, experiment_id: str) -> Dict:
        """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ê²°ê³¼ ì¶”ì¶œ"""
        try:
            # PyTorch ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            checkpoint_paths = [
                self.base_dir / "models" / f"{experiment_id}_best.pth",
                self.base_dir / "checkpoints" / f"{experiment_id}.pth",
                self.base_dir / f"best_model_{experiment_id}.pth"
            ]
            
            for checkpoint_path in checkpoint_paths:
                if checkpoint_path.exists():
                    try:
                        checkpoint = torch.load(checkpoint_path, map_location='cpu')
                        
                        if isinstance(checkpoint, dict):
                            return {
                                "validation_f1": checkpoint.get('val_f1', checkpoint.get('best_f1')),
                                "validation_acc": checkpoint.get('val_acc', checkpoint.get('best_acc')),
                                "training_time_minutes": checkpoint.get('training_time'),
                                "total_epochs": checkpoint.get('epoch'),
                                "early_stopped": checkpoint.get('early_stopped'),
                                "source": f"checkpoint_{checkpoint_path.name}"
                            }
                    except Exception:
                        continue  # ë‹¤ìŒ ì²´í¬í¬ì¸íŠ¸ ì‹œë„
                        
        except Exception as e:
            print(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _execute_experiment(self, experiment: Dict) -> bool:
        """ê°œì„ ëœ ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (ì™„ì „ ê²©ë¦¬)"""
        experiment_id = experiment['experiment_id']
        config_path = experiment['config_path']
        
        print(f"ğŸš€ ì‹¤í—˜ ì‹œì‘: {experiment_id}")
        print(f"   ğŸ“„ ì„¤ì •: {config_path}")
        print(f"   ğŸ“ ì„¤ëª…: {experiment['description']}")
        print(f"   â±ï¸  ì˜ˆìƒ ì‹œê°„: {experiment['estimated_time_minutes']}ë¶„")
        
        # ê²©ë¦¬ëœ í™˜ê²½ì—ì„œ ì‹¤í—˜ ì‹¤í–‰
        with ExperimentIsolationContext(experiment_id):
            return self._run_isolated_experiment(experiment)
        
    def _run_isolated_experiment(self, experiment: Dict) -> bool:
        """ê²©ë¦¬ëœ í™˜ê²½ì—ì„œ ì‹¤í—˜ ì‹¤í–‰"""
        experiment_id = experiment['experiment_id']
        config_path = experiment['config_path']
        
        # ì‹¤í—˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        self._update_experiment_status(experiment_id, 'running', {
            'started_at': datetime.now().isoformat()
        })
        
        try:
            # ê²©ë¦¬ëœ í™˜ê²½ ë³€ìˆ˜ ìƒì„±
            isolated_env = self._create_isolated_environment(experiment_id)
            
            # gemini_main_v2.py ì‹¤í–‰
            main_script = "codes/gemini_main_v2.py"
            cmd = [
                sys.executable, main_script, 
                "--config", config_path,
                "--experiment-id", experiment_id,
                "--isolated-mode"
            ]
            
            print(f"   ğŸ’» ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
            print(f"   ğŸ”’ ê²©ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰")
            
            # ì‹¤í—˜ ì‹¤í–‰ (ê²©ë¦¬ëœ í™˜ê²½)
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                cwd=self.base_dir,
                env=isolated_env  # ê²©ë¦¬ëœ í™˜ê²½ ì ìš©
            )
            
            # ì‹¤ì‹œê°„ ì¶œë ¥ ë° ë¡œê·¸ ìˆ˜ì§‘
            output_lines = []
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    print(f"   ğŸ“¤ {output.strip()}")
                    output_lines.append(output.strip())
            
            # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
            return_code = process.poll()
            
            if return_code == 0:
                print(f"âœ… ì‹¤í—˜ ì„±ê³µ: {experiment_id}")
                
                # ì„±ê³µ ë¡œê·¸ ìƒì„±
                log_file = self._create_experiment_result_log(experiment, True)
                print(f"   ğŸ“Š ê²°ê³¼ ë¡œê·¸: {log_file}")
                
                # ì‹¤í—˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                self._update_experiment_status(experiment_id, 'completed', {
                    'completed_at': datetime.now().isoformat(),
                    'success': True,
                    'log_file': log_file
                })
                
                self.completed_experiments.append(experiment_id)
                return True
            else:
                error_msg = f"í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì½”ë“œ: {return_code}"
                print(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {experiment_id} - {error_msg}")
                
                # ì‹¤íŒ¨ ë¡œê·¸ ìƒì„±
                log_file = self._create_experiment_result_log(experiment, False, error_msg)
                print(f"   ğŸ“‹ ì˜¤ë¥˜ ë¡œê·¸: {log_file}")
                
                # ì‹¤í—˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                self._update_experiment_status(experiment_id, 'failed', {
                    'failed_at': datetime.now().isoformat(),
                    'error_message': error_msg,
                    'log_file': log_file
                })
                
                self.failed_experiments.append(experiment_id)
                return False
                
        except Exception as e:
            error_msg = f"ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"
            print(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {experiment_id} - {error_msg}")
            print(f"   ğŸ” ìƒì„¸ ì˜¤ë¥˜:\\n{traceback.format_exc()}")
            
            # ì‹¤íŒ¨ ë¡œê·¸ ìƒì„±
            log_file = self._create_experiment_result_log(experiment, False, error_msg)
            
            # ì‹¤í—˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            self._update_experiment_status(experiment_id, 'failed', {
                'failed_at': datetime.now().isoformat(),
                'error_message': error_msg,
                'log_file': log_file
            })
            
            self.failed_experiments.append(experiment_id)
            return False
        
        finally:
            # ì‹¤í—˜ í›„ ì² ì €í•œ ì •ë¦¬ëŠ” ExperimentIsolationContextì—ì„œ ì²˜ë¦¬
            pass
    
    def _create_isolated_environment(self, experiment_id: str) -> dict:
        """ê²©ë¦¬ëœ í™˜ê²½ ë³€ìˆ˜ ìƒì„±"""
        env = os.environ.copy()
        
        # ì‹¤í—˜ë³„ ê³ ìœ  í™˜ê²½ ì„¤ì •
        env.update({
            'EXPERIMENT_ID': experiment_id,
            'PYTHONHASHSEED': str(hash(experiment_id) % 2**32),
            'CUDA_CACHE_PATH': f"/tmp/cv_experiments/{experiment_id}/cuda_cache",
            'TORCH_HOME': f"/tmp/cv_experiments/{experiment_id}/torch_home",
            'WANDB_DIR': f"/tmp/cv_experiments/{experiment_id}/wandb",
        })
        
        return env
    
    def _calculate_remaining_time(self, completed_count: int) -> str:
        """ë‚¨ì€ ì‹¤í—˜ ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚°"""
        if not self.start_time or completed_count == 0:
            return "ê³„ì‚° ì¤‘..."
        
        elapsed_time = datetime.now() - self.start_time
        avg_time_per_experiment = elapsed_time / completed_count
        
        remaining_experiments = len(self.experiments) - completed_count
        remaining_time = avg_time_per_experiment * remaining_experiments
        
        eta = datetime.now() + remaining_time
        return f"{eta.strftime('%Y-%m-%d %H:%M:%S')} (ì•½ {remaining_time})"
    
    def run_all_experiments(self):
        """ëª¨ë“  ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰"""
        print("ğŸ¯ ìë™ ì‹¤í—˜ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 80)
        
        # Resume ëª¨ë“œ í™•ì¸
        start_index = self._check_resume_status()
        
        total_experiments = len(self.experiments)
        pending_experiments = [exp for exp in self.experiments[start_index:] if exp['status'] == 'pending']
        
        print(f"ğŸ“Š ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}ê°œ")
        print(f"âœ… ì™„ë£Œëœ ì‹¤í—˜: {start_index}ê°œ")
        print(f"â³ ëŒ€ê¸° ì¤‘ì¸ ì‹¤í—˜: {len(pending_experiments)}ê°œ")
        print(f"â±ï¸  ì˜ˆìƒ ì´ ì†Œìš”ì‹œê°„: {self.queue_data.get('estimated_total_time_hours', 0):.1f}ì‹œê°„")
        print()
        
        if not pending_experiments:
            print("ğŸ‰ ëª¨ë“  ì‹¤í—˜ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        self.start_time = datetime.now()
        
        # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm ì„¤ì •
        progress_bar = tqdm(
            total=len(pending_experiments),
            desc="ì‹¤í—˜ ì§„í–‰",
            unit="exp",
            position=0,
            leave=True
        )
        
        success_count = 0
        failure_count = 0
        
        try:
            for i, experiment in enumerate(pending_experiments):
                current_position = start_index + i + 1
                
                print(f"\\n{'='*80}")
                print(f"ğŸ§ª ì‹¤í—˜ {current_position}/{total_experiments}: {experiment['experiment_id']}")
                print(f"â­ ìš°ì„ ìˆœìœ„: {experiment['priority_score']:.3f}")
                
                # ë‚¨ì€ ì‹œê°„ ê³„ì‚°
                remaining_time = self._calculate_remaining_time(success_count + failure_count)
                print(f"â° ì˜ˆìƒ ì™„ë£Œ: {remaining_time}")
                print("=" * 80)
                
                # í˜„ì¬ ì‹¤í—˜ ì„¤ì •
                self.current_experiment = experiment
                
                # ì‹¤í—˜ ì‹¤í–‰
                success = self._execute_experiment(experiment)
                
                if success:
                    success_count += 1
                else:
                    failure_count += 1
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.update(1)
                progress_bar.set_postfix({
                    'Success': success_count,
                    'Failed': failure_count,
                    'Current': experiment['experiment_id']
                })
                
                # ì‹¤í—˜ ê°„ ê°„ê²©
                print(f"\\nâ¸ï¸  ë‹¤ìŒ ì‹¤í—˜ ì¤€ë¹„ ì¤‘... (5ì´ˆ ëŒ€ê¸°)")
                time.sleep(5)
        
        except KeyboardInterrupt:
            print(f"\\n\\nâ›” ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            print(f"âœ… ì™„ë£Œëœ ì‹¤í—˜: {success_count}ê°œ")
            print(f"âŒ ì‹¤íŒ¨í•œ ì‹¤í—˜: {failure_count}ê°œ")
            print(f"â¸ï¸  ì¤‘ë‹¨ëœ ì§€ì ì—ì„œ ì¬ê°œí•˜ë ¤ë©´ --resume ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”")
            
        finally:
            progress_bar.close()
            self._print_final_summary(success_count, failure_count)
    
    def _print_final_summary(self, success_count: int, failure_count: int):
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        total_time = datetime.now() - self.start_time if self.start_time else timedelta(0)
        
        print("\\n" + "=" * 80)
        print("ğŸ ìë™ ì‹¤í—˜ ì‹œìŠ¤í…œ ì™„ë£Œ!")
        print("=" * 80)
        print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_time}")
        print(f"âœ… ì„±ê³µí•œ ì‹¤í—˜: {success_count}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ ì‹¤í—˜: {failure_count}ê°œ")
        print(f"ğŸ“Š ì„±ê³µë¥ : {success_count/(success_count+failure_count)*100:.1f}%" if (success_count+failure_count) > 0 else "ğŸ“Š ì„±ê³µë¥ : 0%")
        print()
        
        if self.completed_experiments:
            print("ğŸ¯ ì™„ë£Œëœ ì‹¤í—˜ë“¤:")
            for exp_id in self.completed_experiments:
                print(f"   âœ… {exp_id}")
            print()
        
        if self.failed_experiments:
            print("âš ï¸  ì‹¤íŒ¨í•œ ì‹¤í—˜ë“¤:")
            for exp_id in self.failed_experiments:
                print(f"   âŒ {exp_id}")
            print()
        
        print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. ê²°ê³¼ ë¶„ì„: python experiments/results_analyzer.py")
        print("   2. ì œì¶œ ê´€ë¦¬: python experiments/submission_manager.py --list-pending")
        print("   3. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§: python experiments/experiment_monitor.py")


def main():
    parser = argparse.ArgumentParser(description='ìë™ ì‹¤í—˜ ì‹¤í–‰ê¸°')
    parser.add_argument('--queue', '-q',
                       default='experiments/experiment_queue.json',
                       help='ì‹¤í—˜ í JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--resume', '-r', action='store_true',
                       help='ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì‹¤í—˜ ì¬ê°œ')
    parser.add_argument('--dry-run', action='store_true',
                       help='ì‹¤ì œ ì‹¤í—˜ ì‹¤í–‰ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    try:
        # ì‹¤í—˜ ì‹¤í–‰ê¸° ì´ˆê¸°í™”
        runner = ExperimentRunner(args.queue, args.resume)
        
        if args.dry_run:
            print("ğŸ§ª DRY RUN ëª¨ë“œ: ì‹¤ì œ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"ğŸ“‹ ì´ {len(runner.experiments)}ê°œ ì‹¤í—˜ì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        # ìë™ ì‹¤í—˜ ì‹œì‘
        runner.run_all_experiments()
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜:\\n{traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
