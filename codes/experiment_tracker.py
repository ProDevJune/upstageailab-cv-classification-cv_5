"""
ì‹¤í—˜ ê²°ê³¼ ì¶”ì  ë° ë¶„ì„ ë„êµ¬
HPO ì‹¤í—˜ ê²°ê³¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì‹œê°í™”
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from datetime import datetime
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)
plt.rcParams['font.family'] = ['Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ExperimentTracker:
    """ì‹¤í—˜ ê²°ê³¼ ì¶”ì  ë° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, results_path: str = "experiment_results.csv"):
        self.results_path = Path(results_path)
        self.analysis_dir = Path("analysis_results")
        self.analysis_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“Š ì‹¤í—˜ ì¶”ì ê¸° ì´ˆê¸°í™”: {self.results_path}")
    
    def load_results(self) -> pd.DataFrame:
        """ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ"""
        if not self.results_path.exists():
            print(f"âš ï¸  ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.results_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(self.results_path)
        print(f"ğŸ“ˆ ì´ {len(df)}ê°œ ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ")
        return df
    
    def get_summary(self) -> Dict:
        """ì‹¤í—˜ í˜„í™© ìš”ì•½"""
        df = self.load_results()
        
        if df.empty:
            return {
                'total': 0,
                'completed': 0,
                'failed': 0,
                'running': 0,
                'platforms': [],
                'success_rate': 0
            }
        
        summary = {
            'total': len(df),
            'completed': len(df[df['status'] == 'completed']),
            'failed': len(df[df['status'] == 'failed']),
            'running': len(df[df['status'] == 'running']),
            'platforms': df['platform'].unique().tolist(),
            'success_rate': len(df[df['status'] == 'completed']) / len(df) * 100 if len(df) > 0 else 0
        }
        
        # ì™„ë£Œëœ ì‹¤í—˜ë“¤ì˜ í†µê³„
        completed_df = df[df['status'] == 'completed']
        if not completed_df.empty and 'final_f1' in completed_df.columns:
            summary.update({
                'best_f1': completed_df['final_f1'].max(),
                'avg_f1': completed_df['final_f1'].mean(),
                'std_f1': completed_df['final_f1'].std(),
                'avg_training_time': completed_df['training_time_min'].mean() if 'training_time_min' in completed_df.columns else 0
            })
        
        return summary
    
    def print_summary(self):
        """ì‹¤í—˜ ìš”ì•½ ì¶œë ¥"""
        summary = self.get_summary()
        
        print("\nğŸ“Š ì‹¤í—˜ í˜„í™© ìš”ì•½")
        print("=" * 50)
        print(f"ì´ ì‹¤í—˜ ìˆ˜: {summary['total']}")
        print(f"ì™„ë£Œ: {summary['completed']}")
        print(f"ì‹¤íŒ¨: {summary['failed']}")
        print(f"ì‹¤í–‰ ì¤‘: {summary['running']}")
        print(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        
        if 'best_f1' in summary:
            print(f"\nğŸ† ì„±ëŠ¥ í†µê³„")
            print(f"ìµœê³  F1: {summary['best_f1']:.4f}")
            print(f"í‰ê·  F1: {summary['avg_f1']:.4f}")
            print(f"í‘œì¤€í¸ì°¨: {summary['std_f1']:.4f}")
            print(f"í‰ê·  í›ˆë ¨ ì‹œê°„: {summary['avg_training_time']:.1f}ë¶„")
        
        if summary['platforms']:
            print(f"\nğŸ–¥ï¸  ì‚¬ìš©ëœ í”Œë«í¼: {', '.join(summary['platforms'])}")
    
    def get_top_experiments(self, n: int = 10, metric: str = 'final_f1') -> pd.DataFrame:
        """ìƒìœ„ Nê°œ ì‹¤í—˜ ì¡°íšŒ"""
        df = self.load_results()
        completed_df = df[df['status'] == 'completed']
        
        if completed_df.empty or metric not in completed_df.columns:
            print(f"âš ï¸  ì™„ë£Œëœ ì‹¤í—˜ì´ ì—†ê±°ë‚˜ {metric} ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        top_experiments = completed_df.nlargest(n, metric)
        
        # ì¤‘ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        important_cols = ['experiment_id', 'model_name', 'image_size', 'lr', 
                         'batch_size', 'augmentation_level', 'TTA', metric, 'training_time_min']
        available_cols = [col for col in important_cols if col in top_experiments.columns]
        
        return top_experiments[available_cols]
    
    def analyze_hyperparameters(self) -> Dict:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥ ì˜í–¥ ë¶„ì„"""
        df = self.load_results()
        completed_df = df[df['status'] == 'completed']
        
        if completed_df.empty or 'final_f1' not in completed_df.columns:
            print("âš ï¸  ë¶„ì„í•  ì™„ë£Œëœ ì‹¤í—˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        analysis = {}
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„
        if 'model_name' in completed_df.columns:
            model_performance = completed_df.groupby('model_name')['final_f1'].agg(['mean', 'std', 'count'])
            analysis['model_performance'] = model_performance.to_dict('index')
        
        # ì´ë¯¸ì§€ í¬ê¸°ë³„ ì„±ëŠ¥ ë¶„ì„
        if 'image_size' in completed_df.columns:
            size_performance = completed_df.groupby('image_size')['final_f1'].agg(['mean', 'std', 'count'])
            analysis['image_size_performance'] = size_performance.to_dict('index')
        
        # í•™ìŠµë¥ ë³„ ì„±ëŠ¥ ë¶„ì„
        if 'lr' in completed_df.columns:
            lr_performance = completed_df.groupby('lr')['final_f1'].agg(['mean', 'std', 'count'])
            analysis['lr_performance'] = lr_performance.to_dict('index')
        
        # ì¦ê°• ë ˆë²¨ë³„ ì„±ëŠ¥ ë¶„ì„
        if 'augmentation_level' in completed_df.columns:
            aug_performance = completed_df.groupby('augmentation_level')['final_f1'].agg(['mean', 'std', 'count'])
            analysis['augmentation_performance'] = aug_performance.to_dict('index')
        
        # TTA íš¨ê³¼ ë¶„ì„
        if 'TTA' in completed_df.columns:
            tta_performance = completed_df.groupby('TTA')['final_f1'].agg(['mean', 'std', 'count'])
            analysis['tta_performance'] = tta_performance.to_dict('index')
        
        return analysis
    
    def create_visualizations(self, save_plots: bool = True) -> List[str]:
        """ê²°ê³¼ ì‹œê°í™” ê·¸ë˜í”„ ìƒì„±"""
        df = self.load_results()
        completed_df = df[df['status'] == 'completed']
        
        if completed_df.empty:
            print("âš ï¸  ì‹œê°í™”í•  ì™„ë£Œëœ ì‹¤í—˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        plot_files = []
        
        # 1. ì‹¤í—˜ ìƒíƒœ íŒŒì´ ì°¨íŠ¸
        fig, ax = plt.subplots(figsize=(8, 6))
        status_counts = df['status'].value_counts()
        colors = ['#2ecc71', '#e74c3c', '#f39c12']
        ax.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', colors=colors)
        ax.set_title('ì‹¤í—˜ ìƒíƒœ ë¶„í¬', fontsize=16, fontweight='bold')
        
        if save_plots:
            plot_path = self.analysis_dir / 'experiment_status.png'
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plot_files.append(str(plot_path))
        
        plt.close()
        
        # 2. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        if 'model_name' in completed_df.columns and 'final_f1' in completed_df.columns:
            fig, ax = plt.subplots(figsize=(12, 6))
            sns.boxplot(data=completed_df, x='model_name', y='final_f1', ax=ax)
            ax.set_title('ëª¨ë¸ë³„ F1 ì„±ëŠ¥ ë¶„í¬', fontsize=16, fontweight='bold')
            ax.set_xlabel('ëª¨ë¸')
            ax.set_ylabel('F1 Score')
            plt.xticks(rotation=45)
            
            if save_plots:
                plot_path = self.analysis_dir / 'model_performance.png'
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                plot_files.append(str(plot_path))
            
            plt.close()
        
        # 3. í•™ìŠµë¥ ë³„ ì„±ëŠ¥ ë¶„ì„
        if 'lr' in completed_df.columns and 'final_f1' in completed_df.columns:
            fig, ax = plt.subplots(figsize=(10, 6))
            lr_performance = completed_df.groupby('lr')['final_f1'].mean().sort_index()
            ax.plot(range(len(lr_performance)), lr_performance.values, 'o-', linewidth=2, markersize=8)
            ax.set_xticks(range(len(lr_performance)))
            ax.set_xticklabels([f"{lr:.0e}" for lr in lr_performance.index])
            ax.set_title('í•™ìŠµë¥ ë³„ í‰ê·  F1 ì„±ëŠ¥', fontsize=16, fontweight='bold')
            ax.set_xlabel('í•™ìŠµë¥ ')
            ax.set_ylabel('í‰ê·  F1 Score')
            ax.grid(True, alpha=0.3)
            
            if save_plots:
                plot_path = self.analysis_dir / 'lr_performance.png'
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                plot_files.append(str(plot_path))
            
            plt.close()
        
        # 4. í›ˆë ¨ ì‹œê°„ vs ì„±ëŠ¥ ì‚°ì ë„
        if all(col in completed_df.columns for col in ['training_time_min', 'final_f1']):
            fig, ax = plt.subplots(figsize=(10, 6))
            scatter = ax.scatter(completed_df['training_time_min'], completed_df['final_f1'], 
                               c=completed_df['image_size'] if 'image_size' in completed_df.columns else 'blue',
                               alpha=0.7, s=60)
            ax.set_title('í›ˆë ¨ ì‹œê°„ vs F1 ì„±ëŠ¥', fontsize=16, fontweight='bold')
            ax.set_xlabel('í›ˆë ¨ ì‹œê°„ (ë¶„)')
            ax.set_ylabel('F1 Score')
            
            if 'image_size' in completed_df.columns:
                cbar = plt.colorbar(scatter)
                cbar.set_label('ì´ë¯¸ì§€ í¬ê¸°')
            
            ax.grid(True, alpha=0.3)
            
            if save_plots:
                plot_path = self.analysis_dir / 'time_vs_performance.png'
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                plot_files.append(str(plot_path))
            
            plt.close()
        
        # 5. í”Œë«í¼ë³„ ì„±ëŠ¥ ë¹„êµ
        if 'platform' in completed_df.columns and len(completed_df['platform'].unique()) > 1:
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.boxplot(data=completed_df, x='platform', y='final_f1', ax=ax)
            ax.set_title('í”Œë«í¼ë³„ F1 ì„±ëŠ¥ ë¶„í¬', fontsize=16, fontweight='bold')
            ax.set_xlabel('í”Œë«í¼')
            ax.set_ylabel('F1 Score')
            plt.xticks(rotation=45)
            
            if save_plots:
                plot_path = self.analysis_dir / 'platform_performance.png'
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                plot_files.append(str(plot_path))
            
            plt.close()
        
        if plot_files:
            print(f"ğŸ“ˆ {len(plot_files)}ê°œ ì‹œê°í™” íŒŒì¼ ìƒì„± ì™„ë£Œ:")
            for file in plot_files:
                print(f"   ğŸ“Š {file}")
        
        return plot_files
    
    def generate_recommendations(self) -> Dict:
        """ìµœì  ì„¤ì • ì¶”ì²œ"""
        df = self.load_results()
        completed_df = df[df['status'] == 'completed']
        
        if completed_df.empty or 'final_f1' not in completed_df.columns:
            return {'message': 'ì¶”ì²œí•  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}
        
        recommendations = {}
        
        # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜
        best_experiment = completed_df.loc[completed_df['final_f1'].idxmax()]
        recommendations['best_experiment'] = {
            'experiment_id': best_experiment.get('experiment_id'),
            'f1_score': best_experiment.get('final_f1'),
            'model': best_experiment.get('model_name'),
            'image_size': best_experiment.get('image_size'),
            'lr': best_experiment.get('lr'),
            'augmentation': best_experiment.get('augmentation_level'),
            'TTA': best_experiment.get('TTA')
        }
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³„ ìµœì ê°’
        hyperparams = {}
        
        for param in ['model_name', 'image_size', 'lr', 'augmentation_level']:
            if param in completed_df.columns:
                best_value = completed_df.groupby(param)['final_f1'].mean().idxmax()
                hyperparams[param] = best_value
        
        recommendations['optimal_hyperparameters'] = hyperparams
        
        # íš¨ìœ¨ì„± ê¸°ì¤€ ì¶”ì²œ (ì„±ëŠ¥ ëŒ€ë¹„ ë¹ ë¥¸ í›ˆë ¨)
        if 'training_time_min' in completed_df.columns:
            # ì„±ëŠ¥/ì‹œê°„ ë¹„ìœ¨ ê³„ì‚°
            completed_df['efficiency'] = completed_df['final_f1'] / completed_df['training_time_min']
            most_efficient = completed_df.loc[completed_df['efficiency'].idxmax()]
            
            recommendations['most_efficient'] = {
                'experiment_id': most_efficient.get('experiment_id'),
                'f1_score': most_efficient.get('final_f1'),
                'training_time': most_efficient.get('training_time_min'),
                'efficiency_ratio': most_efficient.get('efficiency'),
                'model': most_efficient.get('model_name')
            }
        
        # ìƒìœ„ 5ê°œ ì‹¤í—˜ì˜ ê³µí†µì  ë¶„ì„
        top_5 = completed_df.nlargest(5, 'final_f1')
        
        if len(top_5) >= 3:
            common_patterns = {}
            
            for param in ['model_name', 'augmentation_level', 'TTA']:
                if param in top_5.columns:
                    most_common = top_5[param].mode().iloc[0] if not top_5[param].mode().empty else None
                    if most_common is not None:
                        common_patterns[param] = most_common
            
            recommendations['top_experiments_patterns'] = common_patterns
        
        return recommendations
    
    def export_analysis_report(self, filename: str = None) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë° ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"hpo_analysis_report_{timestamp}.json"
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': self.get_summary(),
            'top_experiments': self.get_top_experiments(10).to_dict('records'),
            'hyperparameter_analysis': self.analyze_hyperparameters(),
            'recommendations': self.generate_recommendations()
        }
        
        report_path = self.analysis_dir / filename
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        return str(report_path)
    
    def cleanup_old_experiments(self, days: int = 7):
        """ì˜¤ë˜ëœ ì‹¤í—˜ ë°ì´í„° ì •ë¦¬"""
        df = self.load_results()
        
        if df.empty or 'timestamp' not in df.columns:
            print("âš ï¸  ì •ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        cutoff_date = datetime.now() - pd.Timedelta(days=days)
        
        old_experiments = df[df['timestamp'] < cutoff_date]
        
        if old_experiments.empty:
            print(f"ğŸ—‘ï¸  {days}ì¼ ì´ì „ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ—‘ï¸  {len(old_experiments)}ê°œ ì˜¤ë˜ëœ ì‹¤í—˜ ë°œê²¬ (ê¸°ì¤€: {days}ì¼ ì „)")
        
        # ì‚¬ìš©ì í™•ì¸
        response = input("ì •ë§ë¡œ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        
        if response.lower() == 'y':
            # ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥
            new_df = df[df['timestamp'] >= cutoff_date]
            new_df.to_csv(self.results_path, index=False)
            print(f"âœ… {len(old_experiments)}ê°œ ì‹¤í—˜ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        else:
            print("âŒ ì‚­ì œ ì·¨ì†Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‹¤í—˜ ê²°ê³¼ ë¶„ì„")
    parser.add_argument('--action', choices=['summary', 'top', 'analyze', 'visualize', 'recommend', 'report', 'cleanup'],
                        default='summary', help='ì‹¤í–‰í•  ì‘ì—…')
    parser.add_argument('--n', type=int, default=10, help='ìƒìœ„ Nê°œ ì‹¤í—˜ (top ì‘ì—…ìš©)')
    parser.add_argument('--days', type=int, default=7, help='ì •ë¦¬í•  ì¼ìˆ˜ (cleanup ì‘ì—…ìš©)')
    
    args = parser.parse_args()
    
    tracker = ExperimentTracker()
    
    if args.action == 'summary':
        tracker.print_summary()
    
    elif args.action == 'top':
        top_experiments = tracker.get_top_experiments(args.n)
        if not top_experiments.empty:
            print(f"\nğŸ† ìƒìœ„ {args.n}ê°œ ì‹¤í—˜:")
            print(top_experiments.to_string(index=False))
        else:
            print("âš ï¸  í‘œì‹œí•  ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    elif args.action == 'analyze':
        analysis = tracker.analyze_hyperparameters()
        if analysis:
            print("\nğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ì„:")
            for category, data in analysis.items():
                print(f"\n{category}:")
                for item, stats in data.items():
                    print(f"  {item}: í‰ê· ={stats['mean']:.4f}, í‘œì¤€í¸ì°¨={stats['std']:.4f}, ê°œìˆ˜={stats['count']}")
    
    elif args.action == 'visualize':
        plot_files = tracker.create_visualizations()
        if plot_files:
            print("âœ… ì‹œê°í™” ì™„ë£Œ")
        else:
            print("âš ï¸  ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    elif args.action == 'recommend':
        recommendations = tracker.generate_recommendations()
        if 'message' in recommendations:
            print(f"âš ï¸  {recommendations['message']}")
        else:
            print("\nğŸ¯ ì¶”ì²œ ì„¤ì •:")
            for category, data in recommendations.items():
                print(f"\n{category}:")
                if isinstance(data, dict):
                    for key, value in data.items():
                        print(f"  {key}: {value}")
    
    elif args.action == 'report':
        report_path = tracker.export_analysis_report()
        print("âœ… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
    
    elif args.action == 'cleanup':
        tracker.cleanup_old_experiments(args.days)

if __name__ == "__main__":
    main()
