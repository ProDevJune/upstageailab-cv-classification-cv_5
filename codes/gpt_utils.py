"""
GPT ë²„ì „ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ (gemini_utils.py ê¸°ë°˜)
í”Œë«í¼ë³„ ìµœì í™” ê¸°ëŠ¥ ì¶”ê°€
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import torch.nn.init as init
import timm
from torch.utils.data import Dataset
from types import SimpleNamespace
import yaml
import random
import numpy as np
import pandas as pd
from PIL import Image
import torch.backends.cudnn as cudnn

def load_config(config_path='./config.yaml'):
    """.yaml ì„¤ì • íŒŒì¼ ì½ê¸°

    :param str config_path: _description_, defaults to './config.yaml'
    :return _type_: _description_
    """
    with open(config_path, 'r') as file:
        cfg = yaml.safe_load(file)
    return SimpleNamespace(**cfg)

def set_seed(seed: int = 256):
    """ëœë¤ì„± ì œì–´ í•¨ìˆ˜

    :param int seed: _description_, defaults to 256
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_platform_optimization(device_type: str):
    """í”Œë«í¼ë³„ PyTorch ìµœì í™” ì„¤ì •"""
    
    if device_type == 'cuda':
        print("ğŸš€ CUDA ìµœì í™” ì„¤ì • ì¤‘...")
        cudnn.benchmark = True  # ì…ë ¥ í¬ê¸°ê°€ ì¼ì •í•œ ê²½ìš° ì„±ëŠ¥ í–¥ìƒ
        cudnn.deterministic = False  # ì„±ëŠ¥ ìš°ì„ , ì¬í˜„ì„± ì•½ê°„ í¬ê¸°
        if hasattr(torch.backends.cuda.matmul, 'allow_tf32'):
            torch.backends.cuda.matmul.allow_tf32 = True  # Ampere GPUì—ì„œ ì„±ëŠ¥ í–¥ìƒ
        
        device = torch.device('cuda')
        
    elif device_type == 'mps':
        print("ğŸ Apple Silicon MPS ìµœì í™” ì„¤ì • ì¤‘...")
        device = torch.device('mps')
        
    else:
        print("ğŸ’» CPU ìµœì í™” ì„¤ì • ì¤‘...")
        # CPU ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™”ëŠ” platform_detectorì—ì„œ ì²˜ë¦¬
        device = torch.device('cpu')
    
    return device

class ImageDataset(Dataset):
    """ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ê°œì„ ëœ ë²„ì „)"""
    
    def __init__(self, df: pd.DataFrame, path, transform=None):
        self.df = df
        self.path = path
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        name, target = self.df.iloc[idx]
        
        try:
            img = Image.open(os.path.join(self.path, name))
            img = img.convert('RGB')  # í™•ì‹¤íˆ RGBë¡œ ë³€í™˜
            
            if self.transform:
                img = self.transform(image=np.array(img))['image']
            
            return img, target
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {name}, ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ì´ë¯¸ì§€ ë°˜í™˜ (ë˜ëŠ” ë‹¤ë¥¸ ì²˜ë¦¬)
            default_img = np.zeros((224, 224, 3), dtype=np.uint8)
            if self.transform:
                default_img = self.transform(image=default_img)['image']
            return default_img, target
    
### Getters
def get_activation(activation_option):
    ACTIVATIONS = {
        'None': None,
        'ReLU': nn.ReLU,
        'LeakyReLU': nn.LeakyReLU,
        'ELU': nn.ELU,
        'SELU': nn.SELU,
        'GELU': nn.GELU,
        'Tanh': nn.Tanh,
        'PReLU': nn.PReLU,
        'SiLU': nn.SiLU,
    }
    return ACTIVATIONS[activation_option]

class TimmWrapper(nn.Module):
    """ê°œì„ ëœ Timm ë˜í¼ (í”Œë«í¼ë³„ ìµœì í™” í¬í•¨)"""
    
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        
        # ë°±ë³¸ ëª¨ë¸ ìƒì„±
        self.backbone = timm.create_model(
            model_name=cfg.model_name,
            pretrained=cfg.pretrained,
            num_classes=0, 
            global_pool='avg',
            act_layer=get_activation(cfg.timm.get('activation', 'None'))
        )
        
        # ì‚¬ìš©ì ì •ì˜ í—¤ë“œ
        if hasattr(cfg, 'custom_layer') and cfg.custom_layer:
            self.dropout = nn.Dropout(p=cfg.custom_layer['drop'])
            self.activation = get_activation(cfg.custom_layer['activation'])()
            self.classifier = nn.Sequential(
                nn.Linear(self.backbone.num_features, 1024),
                nn.BatchNorm1d(1024),
                self.activation,
                self.dropout,
                nn.Linear(1024, 17)
            )
        else:
            # ê¸°ë³¸ ë¶„ë¥˜ í—¤ë“œ
            self.classifier = nn.Linear(self.backbone.num_features, 17)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
        # Fine-tuning ì„¤ì •
        self._setup_fine_tuning()
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        def weight_init(m):
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                if hasattr(self.cfg, 'timm') and self.cfg.timm.get('activation') in ['Tanh']:
                    init.xavier_uniform_(m.weight)
                else:
                    init.kaiming_uniform_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
        
        if self.cfg.fine_tuning == 'scratch':
            self.apply(weight_init)
        elif hasattr(self.cfg, 'custom_layer') and self.cfg.custom_layer:
            self.classifier.apply(weight_init)
    
    def _setup_fine_tuning(self):
        """Fine-tuning ì„¤ì •"""
        if self.cfg.fine_tuning == 'head':
            # ë°±ë³¸ íŒŒë¼ë¯¸í„° freeze
            for param in self.backbone.parameters():
                param.requires_grad = False
        elif self.cfg.fine_tuning == 'custom':
            # ì‚¬ìš©ì ì •ì˜ freeze (êµ¬í˜„ í•„ìš”)
            pass

    def forward(self, x):
        # í”Œë«í¼ë³„ ìµœì í™”ëœ forward
        if hasattr(self.cfg, 'use_channels_last') and self.cfg.use_channels_last:
            x = x.to(memory_format=torch.channels_last)
        
        x = self.backbone(x)
        
        if hasattr(self.cfg, 'custom_layer') and self.cfg.custom_layer:
            x = self.dropout(x)
        
        x = self.classifier(x)
        return x

def get_timm_model(cfg):
    """í”Œë«í¼ë³„ ìµœì í™”ëœ ëª¨ë¸ ìƒì„±"""
    
    if hasattr(cfg, 'custom_layer') and cfg.custom_layer:
        model = TimmWrapper(cfg)
    else:
        model = timm.create_model(
            cfg.model_name,
            pretrained=cfg.pretrained,
            num_classes=17,
            act_layer=get_activation(cfg.timm.get('activation', 'None'))
        )
        
        # ëª¨ë¸ì— ë”°ë¼ 'head'ê°€ ìˆëŠ” ê²ƒë„ ìˆê³  ì—†ëŠ” ê²ƒë„ ìˆë‹¤.
        if hasattr(cfg, 'timm') and cfg.timm.get('head', False):
            if cfg.timm['head'].get('drop'):
                model.head.drop.p = cfg.timm['head']['drop']
    
    # í”Œë«í¼ë³„ ëª¨ë¸ ìµœì í™”
    model = optimize_model_for_platform(model, cfg)
    
    return model.to(cfg.device)

def optimize_model_for_platform(model, cfg):
    """í”Œë«í¼ë³„ ëª¨ë¸ ìµœì í™”"""
    
    device_type = cfg.device.type if hasattr(cfg.device, 'type') else str(cfg.device)
    
    if device_type == 'cuda':
        # CUDA ìµœì í™”
        if hasattr(cfg, 'compile_model') and cfg.compile_model:
            try:
                model = torch.compile(model, mode='reduce-overhead')
                print("âš¡ torch.compile ì ìš© ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸  torch.compile ì‹¤íŒ¨: {e}")
        
        # ë‹¤ì¤‘ GPU ì§€ì›
        if torch.cuda.device_count() > 1 and hasattr(cfg, 'multi_gpu') and cfg.multi_gpu:
            model = nn.DataParallel(model)
            print(f"ğŸ”— DataParallel ì ìš©: {torch.cuda.device_count()}ê°œ GPU")
    
    elif device_type == 'mps':
        # MPS ìµœì í™” (í˜„ì¬ íŠ¹ë³„í•œ ìµœì í™” ì—†ìŒ)
        print("ğŸ MPS ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
    
    elif device_type == 'cpu':
        # CPU ìµœì í™”
        if hasattr(cfg, 'use_channels_last') and cfg.use_channels_last:
            model = model.to(memory_format=torch.channels_last)
            print("âš¡ channels_last ë©”ëª¨ë¦¬ í¬ë§· ì ìš©")
    
    return model

def get_criterion(cfg):
    """ì†ì‹¤ í•¨ìˆ˜ ìƒì„±"""
    CRITERIONS = {
        "CrossEntropyLoss": nn.CrossEntropyLoss()
    }
    return CRITERIONS[cfg.criterion]

def get_optimizer(model, cfg):
    """í”Œë«í¼ë³„ ìµœì í™”ëœ ì˜µí‹°ë§ˆì´ì € ìƒì„±"""
    
    OPTIMIZERS = {
        'SGD': optim.SGD(model.parameters(), lr=cfg.lr),
        'RMSprop': optim.RMSprop(model.parameters(), lr=cfg.lr, alpha=0.99, weight_decay=cfg.weight_decay),
        'Momentum': optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9),
        'NAG': optim.SGD(model.parameters(), lr=cfg.lr, momentum=0.9, nesterov=True),
        'Adam': optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay),
        'AdamW': optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay),
        'NAdam': optim.NAdam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, momentum_decay=4e-3),
        'RAdam': optim.RAdam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay),
    }
    
    try:
        optimizer = OPTIMIZERS[cfg.optimizer_name]
        
        # í”Œë«í¼ë³„ ì˜µí‹°ë§ˆì´ì € ìµœì í™”
        if hasattr(cfg, 'memory_efficient') and cfg.memory_efficient:
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •
            pass
        
        return optimizer
        
    except KeyError:
        print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ì˜µí‹°ë§ˆì´ì €: {cfg.optimizer_name}, Adamìœ¼ë¡œ ëŒ€ì²´")
        return optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

def get_scheduler(optimizer, cfg, steps_per_epoch):
    """í”Œë«í¼ë³„ ìµœì í™”ëœ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±"""
    
    SCHEDULERS = {
        'StepLR': lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1),
        'ExponentialLR': lr_scheduler.ExponentialLR(optimizer, gamma=0.1),
        'CosineAnnealingLR': lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs, eta_min=0),
        'OneCycleLR': lr_scheduler.OneCycleLR(optimizer, max_lr=cfg.lr*10, steps_per_epoch=steps_per_epoch, epochs=cfg.epochs),
        'ReduceLROnPlateau': lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=cfg.patience-5, min_lr=0),
    }
    
    try:
        return SCHEDULERS[cfg.scheduler_name]
    except KeyError:
        print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬: {cfg.scheduler_name}, CosineAnnealingLRìœ¼ë¡œ ëŒ€ì²´")
        return lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs, eta_min=0)
