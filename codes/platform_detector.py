"""
í¬ë¡œìŠ¤ í”Œë«í¼ í™˜ê²½ ìë™ ê°ì§€ ë° ìµœì í™” ëª¨ë“ˆ
Mac MPS / Ubuntu CUDA ìë™ ê°ì§€ ë° ìµœì  ì„¤ì • ì œê³µ
"""

import platform
import torch
import subprocess
import psutil
import os
from typing import Dict, List, Tuple
import json

class PlatformDetector:
    """í¬ë¡œìŠ¤ í”Œë«í¼ í™˜ê²½ ìë™ ê°ì§€ ë° ìµœì í™”"""
    
    def __init__(self):
        self.system_info = self._detect_system()
        self.device_info = self._detect_devices()
        self.optimization_config = self._generate_optimization_config()
    
    def _detect_system(self) -> Dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ê°ì§€"""
        system = platform.system().lower()
        machine = platform.machine().lower()
        
        return {
            'os': system,
            'architecture': machine,
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'cpu_count': psutil.cpu_count(logical=True),
            'memory_gb': round(psutil.virtual_memory().total / (1024**3), 1)
        }
    
    def _detect_devices(self) -> Dict:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í“¨íŒ… ë””ë°”ì´ìŠ¤ ê°ì§€"""
        devices = {
            'cpu': True,
            'cuda': False,
            'mps': False,
            'cuda_devices': [],
            'primary_device': 'cpu'
        }
        
        # CUDA ê°ì§€ (NVIDIA GPU - Linux/Windows)
        if torch.cuda.is_available():
            devices['cuda'] = True
            devices['cuda_devices'] = [
                {
                    'id': i,
                    'name': torch.cuda.get_device_name(i),
                    'memory_gb': round(torch.cuda.get_device_properties(i).total_memory / (1024**3), 1)
                }
                for i in range(torch.cuda.device_count())
            ]
            devices['primary_device'] = 'cuda'
        
        # MPS ê°ì§€ (Apple Silicon - macOS)
        elif torch.backends.mps.is_available():
            devices['mps'] = True
            devices['primary_device'] = 'mps'
            # Apple Silicon ë©”ëª¨ë¦¬ëŠ” í†µí•© ë©”ëª¨ë¦¬
            devices['mps_memory_gb'] = self.system_info['memory_gb']
        
        return devices
    
    def _generate_optimization_config(self) -> Dict:
        """í”Œë«í¼ë³„ ìµœì í™” ì„¤ì • ìƒì„±"""
        config = {
            'device': self.device_info['primary_device'],
            'batch_size_multiplier': 1.0,
            'num_workers': min(4, self.system_info['cpu_count']),
            'pin_memory': True,
            'mixed_precision': False,
            'compile_model': False,
            'memory_efficient': False
        }
        
        # ğŸ§ Linux + CUDA ìµœì í™”
        if self.system_info['os'] == 'linux' and self.device_info['cuda']:
            config.update({
                'batch_size_multiplier': 1.5,  # CUDA ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
                'num_workers': min(8, self.system_info['cpu_count']),
                'mixed_precision': True,  # CUDAì˜ ê°•ë ¥í•œ FP16 ì§€ì›
                'compile_model': True,  # torch.compile ì‚¬ìš©
                'memory_efficient': False
            })
        
        # ğŸ macOS + MPS ìµœì í™”
        elif self.system_info['os'] == 'darwin' and self.device_info['mps']:
            config.update({
                'batch_size_multiplier': 0.8,  # MPS ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤
                'num_workers': min(4, self.system_info['cpu_count'] // 2),  # MPSëŠ” CPUì™€ ë©”ëª¨ë¦¬ ê³µìœ 
                'mixed_precision': False,  # MPS FP16 ì§€ì› ì œí•œì 
                'pin_memory': False,  # MPSì—ì„œëŠ” ë¶ˆí•„ìš”
                'memory_efficient': True  # í†µí•© ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‚¬ìš©
            })
        
        # ğŸ’» CPU ì „ìš© ìµœì í™”
        elif self.device_info['primary_device'] == 'cpu':
            config.update({
                'batch_size_multiplier': 0.5,  # CPU ë©”ëª¨ë¦¬ ì œí•œ
                'num_workers': self.system_info['cpu_count'],
                'mixed_precision': False,
                'memory_efficient': True
            })
        
        return config
    
    def get_recommended_batch_sizes(self, base_batch_size: int = 32) -> Dict[str, int]:
        """í”Œë«í¼ë³„ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°"""
        multiplier = self.optimization_config['batch_size_multiplier']
        base_size = max(1, int(base_batch_size * multiplier))
        
        return {
            'train': base_size,
            'val': base_size * 2,  # ê²€ì¦ì€ gradient ê³„ì‚° ì•ˆí•˜ë¯€ë¡œ ë” í° ë°°ì¹˜ ê°€ëŠ¥
            'test': base_size * 2
        }
    
    def get_hpo_optimization(self) -> Dict:
        """HPOë³„ í”Œë«í¼ ìµœì í™” ì„¤ì •"""
        return {
            'basic_hpo': {
                'max_parallel_trials': 1,  # ì•ˆì „í•œ ì„¤ì •
                'memory_per_trial': 0.8,
                'recommended': self.device_info['primary_device'] == 'cpu'
            },
            'optuna_hpo': {
                'max_parallel_trials': 2 if self.device_info['cuda'] else 1,
                'memory_per_trial': 0.6 if self.device_info['cuda'] else 0.8,
                'pruning_enabled': True,
                'recommended': self.device_info['primary_device'] in ['mps', 'cuda']
            },
            'ray_tune_hpo': {
                'max_parallel_trials': len(self.device_info.get('cuda_devices', [])) or 1,
                'resources_per_trial': self._get_ray_resources(),
                'memory_per_trial': 0.4 if len(self.device_info.get('cuda_devices', [])) > 1 else 0.8,
                'recommended': len(self.device_info.get('cuda_devices', [])) > 1
            }
        }
    
    def _get_ray_resources(self) -> Dict:
        """Ray Tuneìš© ë¦¬ì†ŒìŠ¤ ì„¤ì •"""
        if self.device_info['cuda']:
            gpu_count = len(self.device_info['cuda_devices'])
            return {
                'cpu': 2,
                'gpu': 1 if gpu_count > 1 else 0.5
            }
        elif self.device_info['mps']:
            return {
                'cpu': 4,
                'gpu': 0  # Ray Tuneì´ MPSë¥¼ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŒ
            }
        else:
            return {
                'cpu': max(2, self.system_info['cpu_count'] // 2),
                'gpu': 0
            }
    
    def get_recommended_hpo_method(self) -> str:
        """í˜„ì¬ í”Œë«í¼ì— ìµœì í™”ëœ HPO ë°©ë²• ì¶”ì²œ"""
        hpo_opts = self.get_hpo_optimization()
        
        for method, config in hpo_opts.items():
            if config.get('recommended', False):
                return method.replace('_hpo', '')
        
        # ê¸°ë³¸ê°’: optuna (ê°€ì¥ ë²”ìš©ì )
        return 'optuna'
    
    def print_system_summary(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ìš”ì•½ ì¶œë ¥"""
        print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´")
        print("=" * 50)
        print(f"OS: {self.system_info['os'].title()}")
        print(f"ì•„í‚¤í…ì²˜: {self.system_info['architecture']}")
        print(f"CPU ì½”ì–´: {self.system_info['cpu_count']}")
        print(f"ë©”ëª¨ë¦¬: {self.system_info['memory_gb']} GB")
        print(f"Python: {self.system_info['python_version']}")
        
        print("\nğŸš€ ì»´í“¨íŒ… ë””ë°”ì´ìŠ¤")
        print("=" * 50)
        print(f"ì£¼ ë””ë°”ì´ìŠ¤: {self.device_info['primary_device'].upper()}")
        
        if self.device_info['cuda']:
            print("CUDA ë””ë°”ì´ìŠ¤:")
            for device in self.device_info['cuda_devices']:
                print(f"  - GPU {device['id']}: {device['name']} ({device['memory_gb']} GB)")
        
        if self.device_info['mps']:
            print(f"MPS ë””ë°”ì´ìŠ¤: Apple Silicon ({self.device_info['mps_memory_gb']} GB í†µí•© ë©”ëª¨ë¦¬)")
        
        print("\nâš™ï¸  ìµœì í™” ì„¤ì •")
        print("=" * 50)
        batch_sizes = self.get_recommended_batch_sizes()
        recommended_hpo = self.get_recommended_hpo_method()
        
        print(f"ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {batch_sizes['train']}")
        print(f"ì›Œì»¤ ìˆ˜: {self.optimization_config['num_workers']}")
        print(f"í˜¼í•© ì •ë°€ë„: {self.optimization_config['mixed_precision']}")
        print(f"ê¶Œì¥ HPO ë°©ë²•: {recommended_hpo.upper()}")
        
        print("\nğŸ“Š HPO ìµœì í™” ì •ë³´")
        print("=" * 50)
        hpo_opts = self.get_hpo_optimization()
        for method, config in hpo_opts.items():
            status = "âœ… ê¶Œì¥" if config.get('recommended') else "âšª ê°€ëŠ¥"
            print(f"{status} {method.replace('_hpo', '').upper()}: ë³‘ë ¬ {config['max_parallel_trials']}ê°œ")
    
    def save_platform_info(self, filepath: str = "platform_info.json"):
        """í”Œë«í¼ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        platform_data = {
            'system_info': self.system_info,
            'device_info': self.device_info,
            'optimization_config': self.optimization_config,
            'recommended_hpo': self.get_recommended_hpo_method(),
            'recommended_batch_sizes': self.get_recommended_batch_sizes(),
            'hpo_optimization': self.get_hpo_optimization()
        }
        
        with open(filepath, 'w') as f:
            json.dump(platform_data, f, indent=2)
        
        print(f"ğŸ“ í”Œë«í¼ ì •ë³´ ì €ì¥: {filepath}")

if __name__ == "__main__":
    detector = PlatformDetector()
    detector.print_system_summary()
    detector.save_platform_info()
