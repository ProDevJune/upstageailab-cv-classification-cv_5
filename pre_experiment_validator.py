#!/usr/bin/env python3
"""
ì¢…í•© ì‚¬ì „ ì‹¤í—˜ ê²€ì¦ ì‹œìŠ¤í…œ
Mac OS (MPS) / Ubuntu (CUDA) í™˜ê²½ì—ì„œ ëª¨ë“  ì‹¤í—˜ ì¡°í•©ì´ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ ì‚¬ì „ ê²€ì¦

ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ ì „ì— ê° í™˜ê²½ë³„ë¡œ:
1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ìƒíƒœ í™•ì¸
2. ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ì¸ 
3. ëª¨ë“  ëª¨ë¸Ã—ê¸°ë²•Ã—OCR ì¡°í•© í…ŒìŠ¤íŠ¸
4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
5. ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
"""

import os
import sys
import json
import yaml
import time
import traceback
import subprocess
import tempfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import argparse

# ë™ì ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê°ì§€
project_root = Path(__file__).parent.resolve()
sys.path.append(str(project_root / "codes"))
sys.path.append(str(project_root / "experiments"))

# í•„ìˆ˜ imports
try:
    import torch
    import torch.nn as nn
    import torchvision
    import timm
    import numpy as np
    import pandas as pd
    from tqdm import tqdm
    import psutil
    
    # í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
    from platform_detector import PlatformDetector
    from experiment_generator import ExperimentGenerator
    
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ import ì‹¤íŒ¨: {e}")
    print("ë¨¼ì € setup_platform_env.shë¥¼ ì‹¤í–‰í•˜ì—¬ í™˜ê²½ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    sys.exit(1)


class PreExperimentValidator:
    """ì‚¬ì „ ì‹¤í—˜ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.platform_detector = PlatformDetector()
        self.validation_results = {
            'timestamp': datetime.now().isoformat(),
            'platform_info': {},
            'package_validation': {},
            'device_validation': {},
            'model_validation': {},
            'experiment_validation': {},
            'memory_analysis': {},
            'performance_estimation': {},
            'final_status': 'pending'
        }
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸Ã—ê¸°ë²•Ã—OCR ì¡°í•© (ìƒ˜í”Œ)
        self.test_combinations = self._generate_test_combinations()
        
    def _generate_test_combinations(self) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸í•  í•µì‹¬ ì¡°í•©ë“¤ ìƒì„± (ì „ì²´ 48ê°œ ì¤‘ ëŒ€í‘œì ì¸ 12ê°œ)"""
        models = ['efficientnet_b4', 'swin_transformer']  # ë¹ ë¥¸ ëª¨ë¸ 2ê°œ
        techniques = ['baseline', 'focal_loss', 'mixup_cutmix']  # í•µì‹¬ ê¸°ë²• 3ê°œ
        ocr_options = [False, True]  # OCR ë¯¸ì ìš©/ì ìš©
        
        combinations = []
        for model in models:
            for technique in techniques:
                for ocr in ocr_options:
                    combinations.append({
                        'model': model,
                        'technique': technique,
                        'ocr_enabled': ocr,
                        'test_id': f"test_{model}_{technique}_{'ocr' if ocr else 'noocr'}"
                    })
        
        return combinations
    
    def validate_packages(self) -> Dict:
        """í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë²„ì „ í™•ì¸"""
        print("ğŸ“¦ íŒ¨í‚¤ì§€ ê²€ì¦ ì¤‘...")
        
        # Python ë²„ì „ í™•ì¸
        python_version = sys.version_info
        print(f"   ğŸ Python ë²„ì „: {python_version.major}.{python_version.minor}.{python_version.micro}")
        
        if python_version >= (3, 13):
            print(f"   âš ï¸  Python 3.13+ ê°ì§€ - ì¼ë¶€ íŒ¨í‚¤ì§€ í˜¸í™˜ì„± ì´ìŠˆ ê°€ëŠ¥")
        
        required_packages = {
            'torch': '2.5.0',
            'torchvision': '0.20.0',
            'timm': '1.0.12',
            'numpy': None,  # ë²„ì „ ì²´í¬ ì—†ì´
            'pandas': None,
            'pyyaml': None,  # yaml ëª¨ë“ˆë¡œ ì²´í¬
            'tqdm': None,
            'psutil': None,
            'opencv-python': None,
            'PIL': None  # Pillow
        }
        
        package_status = {}
        all_packages_ok = True
        
        for package, expected_version in required_packages.items():
            try:
                if package == 'PIL':
                    import PIL
                    version = PIL.__version__
                    package_name = 'Pillow'
                elif package == 'opencv-python':
                    import cv2
                    version = cv2.__version__
                    package_name = 'opencv-python'
                elif package == 'pyyaml':
                    import yaml
                    version = yaml.__version__
                    package_name = 'pyyaml'
                else:
                    module = __import__(package)
                    version = getattr(module, '__version__', 'unknown')
                    package_name = package
                
                package_status[package_name] = {
                    'installed': True,
                    'version': version,
                    'expected': expected_version,
                    'status': 'âœ…'
                }
                
                print(f"   âœ… {package_name}: {version}")
                
            except ImportError:
                package_status[package] = {
                    'installed': False,
                    'version': None,
                    'expected': expected_version,
                    'status': 'âŒ'
                }
                all_packages_ok = False
                print(f"   âŒ {package}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        
        return {
            'all_packages_ok': all_packages_ok,
            'packages': package_status,
            'validation_time': datetime.now().isoformat()
        }
    
    def validate_device_compatibility(self) -> Dict:
        """ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ê²€ì¦"""
        print("ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ê²€ì¦ ì¤‘...")
        
        device_status = {
            'primary_device': self.platform_detector.device_info['primary_device'],
            'tests': {},
            'all_tests_passed': True
        }
        
        # 1. ê¸°ë³¸ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        try:
            device = self.platform_detector.device_info['primary_device']
            x = torch.randn(100, 100).to(device)
            y = torch.randn(100, 100).to(device)
            z = torch.mm(x, y)
            
            device_status['tests']['basic_operations'] = {
                'status': 'âœ…',
                'message': f'{device.upper()}ì—ì„œ ê¸°ë³¸ í…ì„œ ì—°ì‚° ì„±ê³µ'
            }
            print(f"   âœ… ê¸°ë³¸ í…ì„œ ì—°ì‚°: {device.upper()}")
            
        except Exception as e:
            device_status['tests']['basic_operations'] = {
                'status': 'âŒ',
                'message': f'ê¸°ë³¸ í…ì„œ ì—°ì‚° ì‹¤íŒ¨: {str(e)}'
            }
            device_status['all_tests_passed'] = False
            print(f"   âŒ ê¸°ë³¸ í…ì„œ ì—°ì‚° ì‹¤íŒ¨: {e}")
        
        # 2. ê°„ë‹¨í•œ ì‹ ê²½ë§ í…ŒìŠ¤íŠ¸
        try:
            device = self.platform_detector.device_info['primary_device']
            model = nn.Sequential(
                nn.Linear(10, 5),
                nn.ReLU(),
                nn.Linear(5, 1)
            ).to(device)
            
            x = torch.randn(32, 10).to(device)
            y = model(x)
            loss = nn.MSELoss()(y, torch.randn(32, 1).to(device))
            loss.backward()
            
            device_status['tests']['neural_network'] = {
                'status': 'âœ…',
                'message': f'{device.upper()}ì—ì„œ ì‹ ê²½ë§ forward/backward ì„±ê³µ'
            }
            print(f"   âœ… ì‹ ê²½ë§ ì—°ì‚°: {device.upper()}")
            
        except Exception as e:
            device_status['tests']['neural_network'] = {
                'status': 'âŒ',
                'message': f'ì‹ ê²½ë§ ì—°ì‚° ì‹¤íŒ¨: {str(e)}'
            }
            device_status['all_tests_passed'] = False
            print(f"   âŒ ì‹ ê²½ë§ ì—°ì‚° ì‹¤íŒ¨: {e}")
        
        # 3. Mixed Precision í…ŒìŠ¤íŠ¸ (CUDAì—ì„œë§Œ)
        if self.platform_detector.device_info['primary_device'] == 'cuda':
            try:
                from torch.cuda.amp import autocast, GradScaler
                
                device = 'cuda'
                model = nn.Linear(100, 10).to(device)
                scaler = GradScaler()
                
                x = torch.randn(32, 100).to(device)
                with autocast():
                    y = model(x)
                    loss = nn.MSELoss()(y, torch.randn(32, 10).to(device))
                
                scaler.scale(loss).backward()
                scaler.step(torch.optim.Adam(model.parameters()))
                scaler.update()
                
                device_status['tests']['mixed_precision'] = {
                    'status': 'âœ…',
                    'message': 'CUDA Mixed Precision ì§€ì›'
                }
                print(f"   âœ… Mixed Precision: CUDA")
                
            except Exception as e:
                device_status['tests']['mixed_precision'] = {
                    'status': 'âš ï¸',
                    'message': f'Mixed Precision ì‹¤íŒ¨ (ì„ íƒì ): {str(e)}'
                }
                print(f"   âš ï¸  Mixed Precision ì‹¤íŒ¨: {e}")
        
        return device_status
    
    def validate_models(self) -> Dict:
        """í•µì‹¬ ëª¨ë¸ë“¤ ë¡œë“œ ë° ê¸°ë³¸ ì—°ì‚° í…ŒìŠ¤íŠ¸"""
        print("ğŸ¤– ëª¨ë¸ ê²€ì¦ ì¤‘...")
        
        test_models = {
            'efficientnet_b4': 'efficientnet_b4.ra2_in1k',
            'swin_transformer': 'swin_base_patch4_window12_384.ms_in1k',
        }
        
        model_status = {
            'models': {},
            'all_models_ok': True
        }
        
        device = self.platform_detector.device_info['primary_device']
        
        for model_key, model_name in test_models.items():
            try:
                print(f"   í…ŒìŠ¤íŠ¸ ì¤‘: {model_key}")
                
                # ëª¨ë¸ ë¡œë“œ
                model = timm.create_model(model_name, pretrained=False, num_classes=42)
                model = model.to(device)
                model.eval()
                
                # í…ŒìŠ¤íŠ¸ ì…ë ¥
                if 'swin' in model_name:
                    test_input = torch.randn(2, 3, 384, 384).to(device)
                else:
                    test_input = torch.randn(2, 3, 320, 320).to(device)
                
                # Forward pass í…ŒìŠ¤íŠ¸
                with torch.no_grad():
                    output = model(test_input)
                
                model_status['models'][model_key] = {
                    'status': 'âœ…',
                    'model_name': model_name,
                    'output_shape': list(output.shape),
                    'memory_mb': self._get_model_memory_usage(model),
                    'device': device
                }
                print(f"      âœ… {model_key}: {output.shape}")
                
            except Exception as e:
                model_status['models'][model_key] = {
                    'status': 'âŒ',
                    'error': str(e),
                    'model_name': model_name
                }
                model_status['all_models_ok'] = False
                print(f"      âŒ {model_key}: {e}")
        
        return model_status
    
    def _get_model_memory_usage(self, model) -> float:
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° (MB)"""
        param_size = sum(p.numel() * p.element_size() for p in model.parameters())
        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
        return (param_size + buffer_size) / 1024 / 1024
    
    def validate_experiment_combinations(self) -> Dict:
        """í•µì‹¬ ì‹¤í—˜ ì¡°í•©ë“¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª ì‹¤í—˜ ì¡°í•© ê²€ì¦ ì¤‘...")
        
        combination_status = {
            'total_combinations': len(self.test_combinations),
            'tested_combinations': 0,
            'successful_combinations': 0,
            'failed_combinations': 0,
            'tests': {},
            'all_combinations_ok': True
        }
        
        device = self.platform_detector.device_info['primary_device']
        
        for i, combo in enumerate(self.test_combinations, 1):
            test_id = combo['test_id']
            print(f"   í…ŒìŠ¤íŠ¸ {i}/{len(self.test_combinations)}: {test_id}")
            
            try:
                # ì„ì‹œ config ìƒì„±
                temp_config = self._create_temp_config(combo)
                
                # ë¹ ë¥¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ (1 epoch, ì‘ì€ ë°ì´í„°)
                result = self._run_quick_training_test(temp_config, device)
                
                combination_status['tests'][test_id] = {
                    'status': 'âœ…',
                    'model': combo['model'],
                    'technique': combo['technique'],
                    'ocr_enabled': combo['ocr_enabled'],
                    'execution_time_seconds': result['execution_time'],
                    'memory_peak_mb': result['memory_peak'],
                    'final_loss': result['final_loss']
                }
                combination_status['successful_combinations'] += 1
                print(f"      âœ… ì„±ê³µ ({result['execution_time']:.1f}ì´ˆ)")
                
            except Exception as e:
                combination_status['tests'][test_id] = {
                    'status': 'âŒ',
                    'error': str(e),
                    'model': combo['model'],
                    'technique': combo['technique'],
                    'ocr_enabled': combo['ocr_enabled']
                }
                combination_status['failed_combinations'] += 1
                combination_status['all_combinations_ok'] = False
                print(f"      âŒ ì‹¤íŒ¨: {e}")
            
            combination_status['tested_combinations'] += 1
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif torch.backends.mps.is_available():
                torch.mps.empty_cache()
        
        return combination_status
    
    def _create_temp_config(self, combo: Dict) -> Dict:
        """í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ config ìƒì„±"""
        # ê¸°ë³¸ config ë¡œë“œ
        base_config_path = self.base_dir / "codes" / "config_v2.yaml"
        with open(base_config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •ìœ¼ë¡œ ìˆ˜ì •
        model_configs = {
            'efficientnet_b4': {
                'model_name': 'efficientnet_b4.ra2_in1k',
                'batch_size': 4,  # ì‘ì€ ë°°ì¹˜
                'image_size': 224  # ì‘ì€ ì´ë¯¸ì§€
            },
            'swin_transformer': {
                'model_name': 'swin_base_patch4_window12_384.ms_in1k',
                'batch_size': 2,  # ì‘ì€ ë°°ì¹˜
                'image_size': 384  # Swinì— ë§ëŠ” ì´ë¯¸ì§€ í¬ê¸°
            }
        }
        
        model_config = model_configs[combo['model']]
        config.update(model_config)
        
        # ê¸°ë²•ë³„ ì„¤ì •
        if combo['technique'] == 'focal_loss':
            config['criterion'] = 'FocalLoss'
            config['focal_loss'] = {'alpha': 1.0, 'gamma': 2.0}
        elif combo['technique'] == 'mixup_cutmix':
            config['criterion'] = 'CrossEntropyLoss'
            config['mixup_cutmix']['prob'] = 0.5
            config['augmentation']['mixup'] = True
            config['augmentation']['cutmix'] = True
        else:  # baseline
            config['criterion'] = 'CrossEntropyLoss'
            config['mixup_cutmix']['prob'] = 0.0
            config['augmentation']['mixup'] = False
            config['augmentation']['cutmix'] = False
        
        # OCR ì„¤ì •
        config['ocr'] = {
            'enabled': combo['ocr_enabled'],
            'description': 'OCR ì ìš©' if combo['ocr_enabled'] else 'OCR ë¯¸ì ìš©'
        }
        
        # í…ŒìŠ¤íŠ¸ìš© ìµœì í™”
        config['epochs'] = 1  # 1 epochë§Œ
        config['patience'] = 1
        config['wandb']['log'] = False  # W&B ë¹„í™œì„±í™”
        
        return config
    
    def _run_quick_training_test(self, config: Dict, device: str) -> Dict:
        """ë¹ ë¥¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # ëª¨ë¸ ìƒì„±
        model = timm.create_model(
            config['model_name'], 
            pretrained=False, 
            num_classes=42
        ).to(device)
        
        # ì†ì‹¤ í•¨ìˆ˜
        if config['criterion'] == 'FocalLoss':
            # ê°„ë‹¨í•œ Focal Loss êµ¬í˜„
            criterion = nn.CrossEntropyLoss()  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ CrossEntropy ì‚¬ìš©
        else:
            criterion = nn.CrossEntropyLoss()
        
        # ì˜µí‹°ë§ˆì´ì €
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        # ë”ë¯¸ ë°ì´í„° ìƒì„±
        batch_size = config['batch_size']
        image_size = config['image_size']
        
        dummy_images = torch.randn(batch_size, 3, image_size, image_size).to(device)
        dummy_labels = torch.randint(0, 42, (batch_size,)).to(device)
        
        # ì§§ì€ í•™ìŠµ ì‹¤í–‰
        model.train()
        
        for step in range(3):  # 3 stepë§Œ ì‹¤í–‰
            optimizer.zero_grad()
            
            # OCR í…ŒìŠ¤íŠ¸ (ì‹¤ì œë¡œëŠ” ë”ë¯¸ ë°ì´í„°)
            if config['ocr']['enabled']:
                # OCR íŠ¹ì„± ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”©)
                ocr_features = torch.randn(batch_size, 768).to(device)
            
            outputs = model(dummy_images)
            loss = criterion(outputs, dummy_labels)
            
            loss.backward()
            optimizer.step()
        
        final_loss = loss.item()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_peak = current_memory - initial_memory
        
        execution_time = time.time() - start_time
        
        return {
            'execution_time': execution_time,
            'memory_peak': memory_peak,
            'final_loss': final_loss
        }
    
    def analyze_memory_requirements(self) -> Dict:
        """ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ë¶„ì„"""
        print("ğŸ’¾ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘...")
        
        device = self.platform_detector.device_info['primary_device']
        memory_analysis = {
            'device': device,
            'system_memory_gb': self.platform_detector.system_info['memory_gb'],
            'model_requirements': {},
            'recommendations': {}
        }
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        if device == 'cuda':
            gpu_memory_gb = self.platform_detector.device_info['cuda_devices'][0]['memory_gb']
            memory_analysis['gpu_memory_gb'] = gpu_memory_gb
        elif device == 'mps':
            memory_analysis['unified_memory_gb'] = self.platform_detector.device_info['mps_memory_gb']
        
        # ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì¶”ì •
        model_memory_estimates = {
            'efficientnet_b4': {'model_mb': 75, 'batch_32_mb': 2500, 'batch_48_mb': 3700},
            'swin_transformer': {'model_mb': 350, 'batch_32_mb': 4500, 'batch_24_mb': 3400},
            'convnext_base': {'model_mb': 340, 'batch_28_mb': 4200, 'batch_32_mb': 4800},
            'maxvit_base': {'model_mb': 470, 'batch_24_mb': 5200, 'batch_28_mb': 6100}
        }
        
        memory_analysis['model_requirements'] = model_memory_estimates
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if device == 'cuda':
            gpu_memory_gb = memory_analysis['gpu_memory_gb']
            if gpu_memory_gb >= 24:
                memory_analysis['recommendations']['status'] = 'âœ… ì¶©ë¶„'
                memory_analysis['recommendations']['message'] = 'ëª¨ë“  ëª¨ë¸ì„ ì›ë˜ ë°°ì¹˜ í¬ê¸°ë¡œ ì‹¤í–‰ ê°€ëŠ¥'
            elif gpu_memory_gb >= 12:
                memory_analysis['recommendations']['status'] = 'âš ï¸ ì£¼ì˜'
                memory_analysis['recommendations']['message'] = 'í° ëª¨ë¸ì€ ë°°ì¹˜ í¬ê¸° ê°ì†Œ í•„ìš”'
            else:
                memory_analysis['recommendations']['status'] = 'âŒ ë¶€ì¡±'
                memory_analysis['recommendations']['message'] = 'ëª¨ë“  ëª¨ë¸ì˜ ë°°ì¹˜ í¬ê¸° ëŒ€í­ ê°ì†Œ í•„ìš”'
        
        elif device == 'mps':
            unified_memory_gb = memory_analysis['unified_memory_gb']
            if unified_memory_gb >= 32:
                memory_analysis['recommendations']['status'] = 'âœ… ì¶©ë¶„'
                memory_analysis['recommendations']['message'] = 'Apple Siliconì—ì„œ ì•ˆì •ì  ì‹¤í–‰ ê°€ëŠ¥'
            elif unified_memory_gb >= 16:
                memory_analysis['recommendations']['status'] = 'âš ï¸ ì£¼ì˜'
                memory_analysis['recommendations']['message'] = 'ë°°ì¹˜ í¬ê¸° ì¡°ì • ê¶Œì¥'
            else:
                memory_analysis['recommendations']['status'] = 'âŒ ë¶€ì¡±'
                memory_analysis['recommendations']['message'] = 'ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì‹¤í–‰ ì–´ë ¤ì›€'
        
        print(f"   {memory_analysis['recommendations']['status']} {memory_analysis['recommendations']['message']}")
        
        return memory_analysis
    
    def estimate_execution_time(self) -> Dict:
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰ ì‹œê°„ ì¶”ì •"""
        print("â±ï¸  ì‹¤í–‰ ì‹œê°„ ì¶”ì • ì¤‘...")
        
        # ì„±ê³µí•œ ì¡°í•©ë“¤ì˜ í‰ê·  ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
        successful_tests = [
            test for test in self.validation_results['experiment_validation']['tests'].values()
            if test['status'] == 'âœ…'
        ]
        
        if not successful_tests:
            return {
                'estimation_available': False,
                'message': 'ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ì–´ ì‹œê°„ ì¶”ì • ë¶ˆê°€'
            }
        
        avg_test_time = sum(test['execution_time_seconds'] for test in successful_tests) / len(successful_tests)
        
        # í…ŒìŠ¤íŠ¸ëŠ” 3 step, ì‹¤ì œëŠ” ì•½ 1000-3000 step ì˜ˆìƒ
        real_experiment_multiplier = 300  # í…ŒìŠ¤íŠ¸ ëŒ€ë¹„ ì‹¤ì œ ì‹¤í—˜ ë¹„ìœ¨
        estimated_time_per_experiment = avg_test_time * real_experiment_multiplier
        
        # ì „ì²´ ì‹¤í—˜ ìˆ˜ (ëª¨ë“œë³„)
        experiment_counts = {
            'none': 24,
            'selective': 32,
            'all': 48
        }
        
        time_estimates = {}
        for mode, count in experiment_counts.items():
            total_seconds = estimated_time_per_experiment * count
            total_hours = total_seconds / 3600
            
            time_estimates[mode] = {
                'total_experiments': count,
                'estimated_seconds_per_experiment': estimated_time_per_experiment,
                'estimated_total_hours': total_hours,
                'estimated_total_days': total_hours / 24,
                'estimated_completion': (datetime.now() + timedelta(seconds=total_seconds)).isoformat()
            }
        
        return {
            'estimation_available': True,
            'base_test_time_seconds': avg_test_time,
            'estimated_real_multiplier': real_experiment_multiplier,
            'time_estimates_by_mode': time_estimates,
            'recommended_mode': 'selective'  # ê· í˜•ì¡íŒ ì„ íƒ
        }
    
    def run_full_validation(self) -> Dict:
        """ì „ì²´ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ” ì¢…í•© ì‚¬ì „ ì‹¤í—˜ ê²€ì¦ ì‹œì‘")
        print("=" * 80)
        
        # í”Œë«í¼ ì •ë³´
        print("ğŸ–¥ï¸  í”Œë«í¼ ì •ë³´:")
        self.platform_detector.print_system_summary()
        self.validation_results['platform_info'] = {
            'system_info': self.platform_detector.system_info,
            'device_info': self.platform_detector.device_info,
            'optimization_config': self.platform_detector.optimization_config
        }
        
        print("\n" + "=" * 80)
        
        # 1. íŒ¨í‚¤ì§€ ê²€ì¦
        self.validation_results['package_validation'] = self.validate_packages()
        
        if not self.validation_results['package_validation']['all_packages_ok']:
            print("âŒ íŒ¨í‚¤ì§€ ê²€ì¦ ì‹¤íŒ¨ - í™˜ê²½ ì„¤ì • í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
            self.validation_results['final_status'] = 'failed_packages'
            return self.validation_results
        
        print("\n" + "-" * 40)
        
        # 2. ë””ë°”ì´ìŠ¤ ê²€ì¦
        self.validation_results['device_validation'] = self.validate_device_compatibility()
        
        if not self.validation_results['device_validation'].get('all_tests_passed', False):
            print("âŒ ë””ë°”ì´ìŠ¤ ê²€ì¦ ì‹¤íŒ¨ - ë“œë¼ì´ë²„ ë˜ëŠ” CUDA/MPS ì„¤ì • í™•ì¸ í•„ìš”")
            self.validation_results['final_status'] = 'failed_device'
            return self.validation_results
        
        print("\n" + "-" * 40)
        
        # 3. ëª¨ë¸ ê²€ì¦
        self.validation_results['model_validation'] = self.validate_models()
        
        if not self.validation_results.get('model_validation', {}).get('all_models_ok', False):
            print("âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨ - ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” ëª¨ë¸ íŒŒì¼ í™•ì¸ í•„ìš”")
            self.validation_results['final_status'] = 'failed_models'
            return self.validation_results
        
        print("\n" + "-" * 40)
        
        # 4. ì‹¤í—˜ ì¡°í•© ê²€ì¦
        self.validation_results['experiment_validation'] = self.validate_experiment_combinations()
        
        if not self.validation_results.get('experiment_validation', {}).get('all_combinations_ok', False):
            print("âš ï¸  ì¼ë¶€ ì‹¤í—˜ ì¡°í•©ì—ì„œ ì˜¤ë¥˜ ë°œìƒ - ì„¸ë¶€ ì‚¬í•­ í™•ì¸ í•„ìš”")
            self.validation_results['final_status'] = 'partial_success'
        else:
            print("âœ… ëª¨ë“  ì‹¤í—˜ ì¡°í•© ê²€ì¦ ì„±ê³µ")
        
        print("\n" + "-" * 40)
        
        # 5. ë©”ëª¨ë¦¬ ë¶„ì„
        self.validation_results['memory_analysis'] = self.analyze_memory_requirements()
        
        print("\n" + "-" * 40)
        
        # 6. ì‹¤í–‰ ì‹œê°„ ì¶”ì •
        self.validation_results['performance_estimation'] = self.estimate_execution_time()
        
        # ìµœì¢… ìƒíƒœ ê²°ì •
        if self.validation_results['final_status'] == 'pending':
            exp_val = self.validation_results.get('experiment_validation', {})
            if exp_val and exp_val.get('total_combinations', 0) > 0:
                successful_rate = (
                    exp_val.get('successful_combinations', 0) /
                    exp_val.get('total_combinations', 1)
                )
                
                if successful_rate >= 0.9:
                    self.validation_results['final_status'] = 'ready'
                elif successful_rate >= 0.7:
                    self.validation_results['final_status'] = 'ready_with_warnings'
                else:
                    self.validation_results['final_status'] = 'not_ready'
            else:
                # ì‹¤í—˜ ê²€ì¦ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
                self.validation_results['final_status'] = 'not_ready'
        
        return self.validation_results
    
    def print_final_summary(self):
        """ìµœì¢… ê²€ì¦ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ ì¢…í•© ê²€ì¦ ê²°ê³¼")
        print("=" * 80)
        
        status = self.validation_results['final_status']
        
        if status == 'ready':
            print("ğŸ‰ ê²€ì¦ ì™„ë£Œ! ëª¨ë“  ì‹¤í—˜ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            status_color = 'âœ…'
        elif status == 'ready_with_warnings':
            print("âš ï¸  ê²€ì¦ ì™„ë£Œ (ì£¼ì˜ì‚¬í•­ ìˆìŒ). ëŒ€ë¶€ë¶„ì˜ ì‹¤í—˜ì€ ì •ìƒ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            status_color = 'âš ï¸'
        else:
            print("âŒ ê²€ì¦ ì‹¤íŒ¨. í™˜ê²½ ì„¤ì • ë˜ëŠ” ì‹œìŠ¤í…œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            status_color = 'âŒ'
        
        print(f"\n{status_color} ìµœì¢… ìƒíƒœ: {status}")
        
        # ì„¸ë¶€ ê²°ê³¼
        validation = self.validation_results
        
        print(f"\nğŸ“¦ íŒ¨í‚¤ì§€: {'âœ…' if validation['package_validation']['all_packages_ok'] else 'âŒ'}")
        print(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {'âœ…' if validation['device_validation'].get('all_tests_passed', False) else 'âŒ'}")
        print(f"ğŸ¤– ëª¨ë¸: {'âœ…' if validation.get('model_validation', {}).get('all_models_ok', False) else 'âŒ'}")
        
        exp_val = validation.get('experiment_validation', {})
        if exp_val:
            success_rate = exp_val.get('successful_combinations', 0) / max(exp_val.get('total_combinations', 1), 1) * 100
            print(f"ğŸ§ª ì‹¤í—˜ ì¡°í•©: {exp_val.get('successful_combinations', 0)}/{exp_val.get('total_combinations', 0)} ({success_rate:.1f}%)")
        else:
            print(f"ğŸ§ª ì‹¤í—˜ ì¡°í•©: ê²€ì¦ë˜ì§€ ì•ŠìŒ")
        
        mem_status = validation.get('memory_analysis', {}).get('recommendations', {}).get('status', 'â“')
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬: {mem_status}")
        
        # ì‹¤í–‰ ì‹œê°„ ì¶”ì •
        perf_est = validation.get('performance_estimation', {})
        if perf_est.get('estimation_available', False):
            estimates = perf_est.get('time_estimates_by_mode', {})
            print(f"\nâ±ï¸  ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„:")
            for mode, est in estimates.items():
                if est:  # estê°€ Noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    print(f"   {mode.upper()}: {est.get('estimated_total_hours', 0):.1f}ì‹œê°„ ({est.get('total_experiments', 0)}ê°œ ì‹¤í—˜)")
        
        # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        if status in ['ready', 'ready_with_warnings']:
            print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
            print(f"   1. python experiments/experiment_generator.py --ocr-mode selective")
            print(f"   2. python experiments/auto_experiment_runner.py")
            print(f"   3. python experiments/experiment_monitor.py")
        else:
            print(f"\nğŸ”§ í•´ê²° ë°©ë²•:")
            if validation['package_validation']['all_packages_ok'] == False:
                print(f"   1. bash setup_platform_env.sh  # í™˜ê²½ ì¬ì„¤ì •")
            if validation.get('device_validation', {}).get('all_tests_passed', False) == False:
                print(f"   2. ë“œë¼ì´ë²„ ë° CUDA/MPS ì„¤ì • í™•ì¸")
            if validation.get('model_validation', {}).get('all_models_ok', False) == False:
                print(f"   3. ì¸í„°ë„· ì—°ê²° ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸")
    
    def save_validation_report(self, output_path: str = None):
        """ê²€ì¦ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.base_dir / f"pre_experiment_validation_{timestamp}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ê²€ì¦ ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='ì¢…í•© ì‚¬ì „ ì‹¤í—˜ ê²€ì¦ ì‹œìŠ¤í…œ')
    parser.add_argument('--base-dir', '-d',
                       default=str(Path(__file__).parent.resolve()),
                       help='í”„ë¡œì íŠ¸ ê¸°ë³¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--save-report', '-s', action='store_true',
                       help='ê²€ì¦ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥')
    parser.add_argument('--quick-test', '-q', action='store_true',
                       help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰ (íŒ¨í‚¤ì§€ + ë””ë°”ì´ìŠ¤ë§Œ)')
    
    args = parser.parse_args()
    
    try:
        validator = PreExperimentValidator(args.base_dir)
        
        if args.quick_test:
            print("ğŸƒ ë¹ ë¥¸ ê²€ì¦ ëª¨ë“œ")
            validator.validation_results['package_validation'] = validator.validate_packages()
            validator.validation_results['device_validation'] = validator.validate_device_compatibility()
            
            if (validator.validation_results['package_validation']['all_packages_ok'] and
                validator.validation_results['device_validation']['all_tests_passed']):
                print("âœ… ë¹ ë¥¸ ê²€ì¦ ì„±ê³µ - ê¸°ë³¸ í™˜ê²½ ì¤€ë¹„ë¨")
            else:
                print("âŒ ë¹ ë¥¸ ê²€ì¦ ì‹¤íŒ¨ - í™˜ê²½ ì„¤ì • í•„ìš”")
        else:
            # ì „ì²´ ê²€ì¦ ì‹¤í–‰
            validator.run_full_validation()
            validator.print_final_summary()
            
            if args.save_report:
                validator.save_validation_report()
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
