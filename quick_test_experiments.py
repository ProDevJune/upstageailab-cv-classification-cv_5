#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ì‹¤í–‰ê¸°
ì‹¤ì œ ê¸´ ì‹¤í—˜ ì „ì— ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¡œ ëª¨ë“  ê²ƒì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
"""

import sys
import os
import yaml
import tempfile
import subprocess
import time
from pathlib import Path
from datetime import datetime

class QuickTestRunner:
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.test_results = []
        
    def create_test_config(self, model_name: str, category: str) -> str:
        """í…ŒìŠ¤íŠ¸ìš© ì„¤ì • íŒŒì¼ ìƒì„± (ë§¤ìš° ì§§ì€ ì‹¤í–‰ ì‹œê°„)"""
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        base_config_path = self.project_root / "codes/config_v2.yaml"
        
        try:
            with open(base_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        except:
            # ê¸°ë³¸ ì„¤ì • ìƒì„±
            config = {
                'data_dir': './data',
                'train_data': 'train0705a.csv',
                'epochs': 1,  # í…ŒìŠ¤íŠ¸ìš©: 1 ì—í¬í¬ë§Œ
                'batch_size': 16,  # ì‘ì€ ë°°ì¹˜
                'image_size': 224,  # ì‘ì€ ì´ë¯¸ì§€
                'lr': 0.001,
                'patience': 1,
                'wandb': {'log': False},  # WandB ë¹„í™œì„±í™”
                'val_split_ratio': 0.8,  # ì‘ì€ validation set
                'mixed_precision': False,  # ì•ˆì •ì„± ìš°ì„ 
                'test_TTA': False,
                'val_TTA': False
            }
        
        # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        test_config = config.copy()
        test_config.update({
            'model_name': model_name,
            'epochs': 2,  # verbose ê²€ì¦ì„ ìœ„í•´ 2 ì—í¬í¬ë¡œ ì„¤ì •
            'batch_size': min(config.get('batch_size', 32), 8),  # ë” ì‘ì€ ë°°ì¹˜ë¡œ ë¹ ë¥¸ ì‹¤í–‰
            'patience': 2,  # epochsì™€ ë§ì¶¤
            'wandb': {'log': False},  # WandB ë¹„í™œì„±í™”
            'val_split_ratio': 0.9,  # validation ë°ì´í„° ë” ìµœì†Œí™” (90% train, 10% val)
            'experiment_id': f"test_{category}_{model_name.replace('.', '_')}_{datetime.now().strftime('%H%M%S')}",
            'mixed_precision': False,  # ì•ˆì •ì„± ìš°ì„ 
            'class_imbalance': False,  # í…ŒìŠ¤íŠ¸ìš©: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ë¹„í™œì„±í™”
            'online_augmentation': False,  # í…ŒìŠ¤íŠ¸ìš©: ì¦ê°• ë¹„í™œì„±í™”
            'dynamic_augmentation': {'enabled': False},  # ë™ì  ì¦ê°• ë¹„í™œì„±í™”
            'val_TTA': False,  # í…ŒìŠ¤íŠ¸ìš©: TTA ë¹„í™œì„±í™”
            'test_TTA': False  # í…ŒìŠ¤íŠ¸ìš©: TTA ë¹„í™œì„±í™”
        })
        
        # ì¹´í…Œê³ ë¦¬ë³„ í…ŒìŠ¤íŠ¸ ì„¤ì •
        if category == 'optimizer':
            test_config['optimizer_name'] = 'SGD'
            test_config['lr'] = 0.01
        elif category == 'loss_function':
            test_config['criterion'] = 'CrossEntropyLoss'
        elif category == 'scheduler':
            test_config['scheduler_name'] = 'StepLR'
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
            yaml.dump(test_config, f, default_flow_style=False)
            return f.name
    
    def run_quick_test(self, model_name: str, category: str) -> dict:
        """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ì‹¤í–‰"""
        
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í—˜: {model_name} + {category}")
        print("-" * 50)
        
        result = {
            'model': model_name,
            'category': category,
            'status': 'running',
            'start_time': datetime.now(),
            'error': None,
            'execution_time': 0
        }
        
        try:
            # í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ìƒì„±
            test_config_path = self.create_test_config(model_name, category)
            print(f"   ğŸ“„ í…ŒìŠ¤íŠ¸ ì„¤ì •: {test_config_path}")
            
            # PYTHONPATHì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ (í™˜ê²½ë³€ìˆ˜)
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.project_root) + ':' + env.get('PYTHONPATH', '')
            
            # ì‹¤í–‰ ëª…ë ¹ ê²°ì •
            if model_name == 'resnet50.tv2_in1k':
                cmd = ['python', 'codes/gemini_main_v2.py', '--config', test_config_path]
            else:
                # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ gemini_main_v2.py ì‚¬ìš©
                cmd = ['python', 'codes/gemini_main_v2.py', '--config', test_config_path]
            
            print(f"   ğŸ”§ ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
            print(f"   â³ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘... (1-3ë¶„ ì˜ˆìƒ)")
            
            # ì‹¤í—˜ ì‹¤í–‰
            start_time = time.time()
            process = subprocess.run(
                cmd,
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=300,  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                env=env  # ìˆ˜ì •ëœ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
            )
            end_time = time.time()
            
            execution_time = end_time - start_time
            result['execution_time'] = execution_time
            
            if process.returncode == 0:
                print(f"   âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! ({execution_time:.1f}ì´ˆ)")
                result['status'] = 'success'
            else:
                print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ì½”ë“œ: {process.returncode})")
                result['status'] = 'failed'
                # ì „ì²´ ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
                full_error = process.stderr if process.stderr else process.stdout
                result['error'] = full_error
                print(f"   ğŸš¨ ì „ì²´ ì˜¤ë¥˜ ë©”ì‹œì§€:")
                print(f"   {full_error}")
                
                # ì˜¤ë¥˜ë¥¼ íŒŒì¼ë¡œë„ ì €ì¥
                error_file = f"/tmp/test_error_{model_name.replace('.', '_')}_{category}.log"
                with open(error_file, 'w') as f:
                    f.write(f"ëª…ë ¹: {' '.join(cmd)}\n")
                    f.write(f"STDOUT:\n{process.stdout}\n")
                    f.write(f"STDERR:\n{process.stderr}\n")
                print(f"   ğŸ“„ ìƒì„¸ ì˜¤ë¥˜ ë¡œê·¸: {error_file}")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(test_config_path)
            except:
                pass
                
        except subprocess.TimeoutExpired:
            result['status'] = 'timeout'
            result['error'] = 'í…ŒìŠ¤íŠ¸ ì‹œê°„ ì´ˆê³¼ (5ë¶„)'
            print(f"   â° í…ŒìŠ¤íŠ¸ ì‹œê°„ ì´ˆê³¼")
            
        except Exception as e:
            result['status'] = 'error'
            result['error'] = str(e)
            print(f"   âŒ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        
        return result
    
    def run_comprehensive_test(self) -> bool:
        """í¬ê´„ì ì¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
        
        print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ì‹œìŠ¤í…œ")
        print("=" * 60)
        print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ì‹œì‘")
        print("=" * 60)
        print("â„¹ï¸ ì‹¤ì œ ê¸´ ì‹¤í—˜ ì „ì— ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
        print("â±ï¸ ê° í…ŒìŠ¤íŠ¸ëŠ” 1-3ë¶„ ì†Œìš”ë˜ë©°, ì „ì²´ í…ŒìŠ¤íŠ¸ëŠ” 5-10ë¶„ ì˜ˆìƒë©ë‹ˆë‹¤.")
        
        # í…ŒìŠ¤íŠ¸í•  ì¡°í•©ë“¤ (ëŒ€í‘œì ì¸ ê²ƒë“¤ë§Œ)
        test_combinations = [
            ('resnet50.tv2_in1k', 'optimizer'),
            ('efficientnet_b4.ra2_in1k', 'loss_function'),
            ('efficientnet_b3.ra2_in1k', 'scheduler'),
        ]
        
        all_tests_passed = True
        
        for model, category in test_combinations:
            result = self.run_quick_test(model, category)
            self.test_results.append(result)
            
            if result['status'] != 'success':
                all_tests_passed = False
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 40)
        
        successful_tests = [r for r in self.test_results if r['status'] == 'success']
        failed_tests = [r for r in self.test_results if r['status'] != 'success']
        
        print(f"   ì„±ê³µ: {len(successful_tests)}/{len(self.test_results)}ê°œ")
        print(f"   í‰ê·  ì‹¤í–‰ ì‹œê°„: {sum(r['execution_time'] for r in successful_tests) / len(successful_tests):.1f}ì´ˆ" if successful_tests else "   í‰ê·  ì‹¤í–‰ ì‹œê°„: N/A")
        
        if failed_tests:
            print(f"\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤:")
            for test in failed_tests:
                print(f"   â€¢ {test['model']} + {test['category']}: {test['error'][:50]}...")
        
        if all_tests_passed:
            print(f"\nğŸŠ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ë³¸ê²©ì ì¸ ì‹¤í—˜ ì‹¤í–‰ ê°€ëŠ¥")
            return True
        else:
            print(f"\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë¬¸ì œ í•´ê²° í›„ ì¬ì‹œë„ í•„ìš”")
            return False
    
    def estimate_full_experiment_time(self) -> dict:
        """ì „ì²´ ì‹¤í—˜ ì‹œê°„ ì¶”ì •"""
        
        if not self.test_results:
            return {'error': 'í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        successful_tests = [r for r in self.test_results if r['status'] == 'success']
        if not successful_tests:
            return {'error': 'ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # í‰ê·  í…ŒìŠ¤íŠ¸ ì‹œê°„ (1 ì—í¬í¬)
        avg_test_time = sum(r['execution_time'] for r in successful_tests) / len(successful_tests)
        
        # ì‹¤ì œ ì‹¤í—˜ ì‹œê°„ ì¶”ì • (50 ì—í¬í¬ ê¸°ì¤€)
        estimated_time_per_experiment = avg_test_time * 50
        
        # ì „ì²´ ì‹¤í—˜ ìˆ˜ ì¶”ì • (4ëª¨ë¸ Ã— 6ì¹´í…Œê³ ë¦¬ Ã— í‰ê·  2.5ì˜µì…˜)
        total_experiments = 4 * 6 * 2.5
        total_estimated_time = estimated_time_per_experiment * total_experiments
        
        return {
            'test_time_per_epoch': avg_test_time,
            'estimated_time_per_experiment': estimated_time_per_experiment / 60,  # ë¶„
            'total_experiments': total_experiments,
            'total_estimated_hours': total_estimated_time / 3600,
            'total_estimated_days': total_estimated_time / (3600 * 24)
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    runner = QuickTestRunner()
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = runner.run_comprehensive_test()
    
    if success:
        # ì „ì²´ ì‹¤í—˜ ì‹œê°„ ì¶”ì •
        estimation = runner.estimate_full_experiment_time()
        
        if 'error' not in estimation:
            print(f"\nâ±ï¸ ì „ì²´ ì‹¤í—˜ ì‹œê°„ ì¶”ì •:")
            print(f"   ì‹¤í—˜ë‹¹ ì˜ˆìƒ ì‹œê°„: {estimation['estimated_time_per_experiment']:.1f}ë¶„")
            print(f"   ì´ ì‹¤í—˜ ìˆ˜: {estimation['total_experiments']:.0f}ê°œ")
            print(f"   ì´ ì˜ˆìƒ ì‹œê°„: {estimation['total_estimated_hours']:.1f}ì‹œê°„ ({estimation['total_estimated_days']:.1f}ì¼)")
        
        print(f"\nğŸš€ ì‹¤ì œ ì‹¤í—˜ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ!")
        print(f"   python hyperparameter_system/run_experiments.py")
        
        return 0
    else:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë¬¸ì œ í•´ê²° í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
        return 1

if __name__ == "__main__":
    exit(main())
