#!/usr/bin/env python3
"""
V2_1 & V2_2 Enhanced Experiment Monitor
ì‹¤ì‹œê°„ ì‹¤í—˜ ëª¨ë‹ˆí„°ë§ ë° ê²°ê³¼ ë¶„ì„
"""

import os
import json
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
from datetime import datetime
import subprocess

class V2ExperimentMonitor:
    def __init__(self, experiment_dir="v2_experiments"):
        self.experiment_dir = Path(experiment_dir)
        self.results_dir = Path("data/submissions")
        self.experiment_list = self.load_experiment_list()
        
    def load_experiment_list(self):
        """ì‹¤í—˜ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
        list_file = self.experiment_dir / "experiment_list.json"
        if list_file.exists():
            with open(list_file) as f:
                return json.load(f)
        return []
    
    def monitor_realtime(self, refresh_interval=30):
        """ì‹¤ì‹œê°„ ì‹¤í—˜ ëª¨ë‹ˆí„°ë§"""
        print("ğŸ” V2_1 & V2_2 Real-time Experiment Monitor")
        print("=" * 50)
        
        while True:
            try:
                self.display_status()
                time.sleep(refresh_interval)
                os.system('clear' if os.name == 'posix' else 'cls')
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Monitoring stopped by user")
                break
    
    def display_status(self):
        """í˜„ì¬ ì‹¤í—˜ ìƒíƒœ í‘œì‹œ"""
        print(f"ğŸ“Š Experiment Status - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        status_summary = self.get_experiment_status()
        
        # ì „ì²´ í†µê³„
        total = len(self.experiment_list)
        completed = status_summary['completed']
        running = status_summary['running']
        pending = status_summary['pending']
        failed = status_summary['failed']
        
        print(f"ğŸ“ˆ Total Experiments: {total}")
        print(f"âœ… Completed: {completed} ({completed/total*100:.1f}%)")
        print(f"ğŸ”„ Running: {running}")
        print(f"â³ Pending: {pending}")
        print(f"âŒ Failed: {failed}")
        print()
        
        # íƒ€ì…ë³„ í†µê³„
        print("ğŸ“Š By Experiment Type:")
        type_stats = status_summary['by_type']
        for exp_type, stats in type_stats.items():
            print(f"  {exp_type.upper()}: {stats['completed']}/{stats['total']} completed")
        print()
        
        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜
        if status_summary['current_experiment']:
            print(f"ğŸ”¬ Currently Running: {status_summary['current_experiment']}")
        
        # ìµœê·¼ ì™„ë£Œëœ ì‹¤í—˜
        recent_completed = status_summary['recent_completed'][:3]
        if recent_completed:
            print("ğŸ† Recently Completed:")
            for exp in recent_completed:
                print(f"  - {exp}")
        
        print("\n" + "=" * 60)
    
    def get_experiment_status(self):
        """ì‹¤í—˜ ìƒíƒœ ì •ë³´ ìˆ˜ì§‘"""
        completed_experiments = []
        running_experiments = []
        failed_experiments = []
        
        # ì™„ë£Œëœ ì‹¤í—˜ (submission íŒŒì¼ ì¡´ì¬)
        for submission_dir in self.results_dir.glob("*"):
            if submission_dir.is_dir():
                exp_name = submission_dir.name
                # submission CSV íŒŒì¼ì´ ìˆìœ¼ë©´ ì™„ë£Œ
                csv_files = list(submission_dir.glob("*.csv"))
                if csv_files:
                    completed_experiments.append(exp_name)
        
        # ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜ (í”„ë¡œì„¸ìŠ¤ í™•ì¸)
        try:
            result = subprocess.run(['pgrep', '-f', 'gemini_main'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                # ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ìˆìŒ
                ps_result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
                for line in ps_result.stdout.split('\n'):
                    if 'gemini_main' in line and 'python' in line:
                        # config íŒŒì¼ëª…ì—ì„œ ì‹¤í—˜ëª… ì¶”ì¶œ
                        for part in line.split():
                            if '.yaml' in part and 'config' in part:
                                config_name = Path(part).stem
                                if config_name not in completed_experiments:
                                    running_experiments.append(config_name)
        except:
            pass
        
        # íƒ€ì…ë³„ í†µê³„
        type_stats = {'v2_1': {'total': 0, 'completed': 0},
                     'v2_2': {'total': 0, 'completed': 0},
                     'cv': {'total': 0, 'completed': 0}}
        
        for exp in self.experiment_list:
            exp_type = exp['type']
            type_stats[exp_type]['total'] += 1
            if exp['name'] in completed_experiments:
                type_stats[exp_type]['completed'] += 1
        
        total_experiments = len(self.experiment_list)
        pending = total_experiments - len(completed_experiments) - len(running_experiments)
        
        return {
            'completed': len(completed_experiments),
            'running': len(running_experiments),
            'pending': max(0, pending),
            'failed': 0,  # ì‹¤íŒ¨ ê°ì§€ ë¡œì§ í•„ìš”ì‹œ ì¶”ê°€
            'by_type': type_stats,
            'current_experiment': running_experiments[0] if running_experiments else None,
            'recent_completed': sorted(completed_experiments, reverse=True)
        }
    
    def analyze_results(self, save_plots=True):
        """ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
        print("ğŸ“Š Analyzing V2_1 & V2_2 Experiment Results")
        print("=" * 50)
        
        results_data = self.collect_results()
        
        if not results_data:
            print("âŒ No experiment results found")
            return
        
        df = pd.DataFrame(results_data)
        
        # ê¸°ë³¸ í†µê³„
        print(f"ğŸ“ˆ Total analyzed experiments: {len(df)}")
        print(f"ğŸ† Best F1 Score: {df['f1_score'].max():.4f}")
        print(f"ğŸ“Š Average F1 Score: {df['f1_score'].mean():.4f}")
        print()
        
        # íƒ€ì…ë³„ ì„±ëŠ¥ ë¹„êµ
        print("ğŸ”¬ Performance by Experiment Type:")
        type_performance = df.groupby('type')['f1_score'].agg(['count', 'mean', 'max']).round(4)
        print(type_performance)
        print()
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        if 'model' in df.columns:
            print("ğŸ—ï¸ Performance by Model:")
            model_performance = df.groupby('model')['f1_score'].agg(['count', 'mean', 'max']).round(4)
            print(model_performance)
            print()
        
        # ì‹œê°í™”
        if save_plots:
            self.create_analysis_plots(df)
        
        # ë² ìŠ¤íŠ¸ ì‹¤í—˜ë“¤
        print("ğŸ¥‡ Top 5 Experiments:")
        top_experiments = df.nlargest(5, 'f1_score')[['name', 'type', 'f1_score']]
        for idx, row in top_experiments.iterrows():
            print(f"  {row['f1_score']:.4f} - {row['name']} ({row['type']})")
    
    def collect_results(self):
        """ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘"""
        results = []
        
        for submission_dir in self.results_dir.glob("*"):
            if not submission_dir.is_dir():
                continue
                
            exp_name = submission_dir.name
            
            # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì°¾ê¸°
            exp_meta = None
            for exp in self.experiment_list:
                if exp['name'] in exp_name:
                    exp_meta = exp
                    break
            
            if not exp_meta:
                continue
            
            # F1 ìŠ¤ì½”ì–´ ì¶”ì¶œ (ë¡œê·¸ íŒŒì¼ì´ë‚˜ ê²°ê³¼ íŒŒì¼ì—ì„œ)
            f1_score = self.extract_f1_score(submission_dir)
            
            if f1_score is not None:
                result = {
                    'name': exp_name,
                    'type': exp_meta['type'],
                    'f1_score': f1_score,
                }
                
                # ëª¨ë¸ëª… ì¶”ì¶œ
                if 'model_name' in exp_meta['overrides']:
                    model_name = exp_meta['overrides']['model_name']
                    result['model'] = model_name.split('.')[0]  # ê°„ë‹¨í•œ ì´ë¦„ë§Œ
                
                results.append(result)
        
        return results
    
    def extract_f1_score(self, submission_dir):
        """F1 ìŠ¤ì½”ì–´ ì¶”ì¶œ"""
        # ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ F1 ìŠ¤ì½”ì–´ ì°¾ê¸°
        
        # 1. ë¡œê·¸ íŒŒì¼ì—ì„œ ì°¾ê¸°
        log_files = list(submission_dir.glob("*.log"))
        for log_file in log_files:
            try:
                with open(log_file, 'r') as f:
                    content = f.read()
                    # "Validation F1-score: 0.xxxx" íŒ¨í„´ ì°¾ê¸°
                    import re
                    match = re.search(r'Validation F1-score:\s*([0-9]+\.?[0-9]*)', content)
                    if match:
                        return float(match.group(1))
            except:
                continue
        
        # 2. ê²°ê³¼ JSON íŒŒì¼ì—ì„œ ì°¾ê¸° (ìˆë‹¤ë©´)
        json_files = list(submission_dir.glob("*.json"))
        for json_file in json_files:
            try:
                with open(json_file, 'r') as f:
                    data = json.load(f)
                    if 'f1_score' in data:
                        return float(data['f1_score'])
            except:
                continue
        
        # 3. ê¸°ë³¸ê°’ (ì„ì˜ì˜ ê°’, ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì¶”ì¶œ í•„ìš”)
        return None
    
    def create_analysis_plots(self, df):
        """ë¶„ì„ í”Œë¡¯ ìƒì„±"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # íƒ€ì…ë³„ ì„±ëŠ¥ ë¶„í¬
        sns.boxplot(data=df, x='type', y='f1_score', ax=axes[0,0])
        axes[0,0].set_title('Performance Distribution by Experiment Type')
        axes[0,0].set_ylabel('F1 Score')
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ (ëª¨ë¸ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        if 'model' in df.columns:
            sns.boxplot(data=df, x='model', y='f1_score', ax=axes[0,1])
            axes[0,1].set_title('Performance Distribution by Model')
            axes[0,1].set_ylabel('F1 Score')
            axes[0,1].tick_params(axis='x', rotation=45)
        
        # ì‹œê°„ìˆœ ì„±ëŠ¥ ì¶”ì´
        df_sorted = df.sort_values('name')  # ì´ë¦„ìœ¼ë¡œ ì •ë ¬ (ì‹œê°„ìˆœ ê°€ì •)
        axes[1,0].plot(range(len(df_sorted)), df_sorted['f1_score'], 'o-')
        axes[1,0].set_title('Performance Trend Over Time')
        axes[1,0].set_xlabel('Experiment Order')
        axes[1,0].set_ylabel('F1 Score')
        
        # ì„±ëŠ¥ íˆìŠ¤í† ê·¸ë¨
        axes[1,1].hist(df['f1_score'], bins=20, alpha=0.7)
        axes[1,1].set_title('F1 Score Distribution')
        axes[1,1].set_xlabel('F1 Score')
        axes[1,1].set_ylabel('Frequency')
        
        plt.tight_layout()
        
        # í”Œë¡¯ ì €ì¥
        plot_path = self.experiment_dir / "analysis_plots.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š Analysis plots saved to {plot_path}")
        plt.close()

def main():
    parser = argparse.ArgumentParser(description="V2_1 & V2_2 Experiment Monitor")
    parser.add_argument('--mode', choices=['monitor', 'analyze'], default='monitor',
                      help='Mode: monitor (real-time) or analyze (results)')
    parser.add_argument('--experiment-dir', default='v2_experiments',
                      help='Experiment directory')
    parser.add_argument('--refresh', type=int, default=30,
                      help='Refresh interval for monitoring (seconds)')
    
    args = parser.parse_args()
    
    monitor = V2ExperimentMonitor(args.experiment_dir)
    
    if args.mode == 'monitor':
        monitor.monitor_realtime(args.refresh)
    elif args.mode == 'analyze':
        monitor.analyze_results()

if __name__ == "__main__":
    main()
