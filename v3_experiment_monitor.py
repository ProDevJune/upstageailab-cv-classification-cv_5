#!/usr/bin/env python3
"""
V3 Hierarchical Classification Experiment Monitor
V3 ê³„ì¸µì  ë¶„ë¥˜ ì‹¤í—˜ ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ
"""

import os
import json
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
from datetime import datetime
import subprocess
import glob
import yaml
import numpy as np

class V3ExperimentMonitor:
    def __init__(self, experiment_dir="v3_experiments"):
        self.experiment_dir = Path(experiment_dir)
        self.results_dir = Path("data/submissions")
        self.experiment_list = self.load_experiment_list()
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.analysis_dir = self.experiment_dir / "analysis"
        self.analysis_dir.mkdir(exist_ok=True)
        
    def load_experiment_list(self):
        """ì‹¤í—˜ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
        list_file = self.experiment_dir / "experiment_list.json"
        if list_file.exists():
            with open(list_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def monitor_realtime(self, refresh_interval=60):
        """ì‹¤ì‹œê°„ V3 ì‹¤í—˜ ëª¨ë‹ˆí„°ë§"""
        print("ğŸ¯ V3 Hierarchical Classification Real-time Monitor")
        print("=" * 70)
        
        while True:
            try:
                self.display_v3_status()
                time.sleep(refresh_interval)
                os.system('clear' if os.name == 'posix' else 'cls')
            except KeyboardInterrupt:
                print("\nğŸ‘‹ V3 Monitoring stopped by user")
                break
    
    def display_v3_status(self):
        """V3 ì‹¤í—˜ ìƒíƒœ í‘œì‹œ"""
        print(f"ğŸ“Š V3 Hierarchical Experiment Status - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
        
        status_summary = self.get_v3_experiment_status()
        
        # ì „ì²´ í†µê³„
        total = len(self.experiment_list)
        completed = status_summary['completed']
        running = status_summary['running']
        pending = status_summary['pending']
        failed = status_summary['failed']
        
        print(f"ğŸ“ˆ Total V3 Experiments: {total}")
        print(f"âœ… Completed: {completed} ({completed/total*100:.1f}%)")
        print(f"ğŸ”„ Running: {running}")
        print(f"â³ Pending: {pending}")
        print(f"âŒ Failed: {failed}")
        print()
        
        # ê³„ì¸µì  ë¶„ë¥˜ íŠ¹í™” ì •ë³´
        print("ğŸ¯ Hierarchical Classification Analysis:")
        print(f"ğŸ“Š Model A Completed: {status_summary['model_a_completed']}")
        print(f"âš¡ Model B Completed: {status_summary['model_b_completed']}")
        print(f"ğŸ”— Full Pipeline Completed: {status_summary['pipeline_completed']}")
        print()
        
        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜
        if status_summary['current_experiment']:
            print(f"ğŸ”¬ Currently Running: {status_summary['current_experiment']}")
        
        # ìµœê·¼ ì™„ë£Œëœ ì‹¤í—˜
        recent_completed = status_summary['recent_completed'][:3]
        if recent_completed:
            print("\nğŸ† Recently Completed:")
            for exp in recent_completed:
                print(f"  âœ… {exp['name']} - F1: {exp.get('f1_score', 'N/A')}")
        
        # ì„±ëŠ¥ ë¶„ì„ (ì™„ë£Œëœ ì‹¤í—˜ì´ ìˆëŠ” ê²½ìš°)
        if completed > 0:
            self.display_performance_summary()
    
    def get_v3_experiment_status(self):
        """V3 ì‹¤í—˜ ìƒíƒœ ìˆ˜ì§‘"""
        status = {
            'completed': 0,
            'running': 0,
            'pending': 0,
            'failed': 0,
            'model_a_completed': 0,
            'model_b_completed': 0,
            'pipeline_completed': 0,
            'current_experiment': None,
            'recent_completed': []
        }
        
        # ì œì¶œ íŒŒì¼ë“¤ í™•ì¸
        submission_files = list(self.results_dir.glob("**/v3_*.csv"))
        completed_experiments = set()
        
        for file_path in submission_files:
            exp_name = self.extract_experiment_name(file_path.name)
            if exp_name:
                completed_experiments.add(exp_name)
        
        # ë¡œê·¸ íŒŒì¼ í™•ì¸
        log_files = list(self.experiment_dir.glob("logs/*.log"))
        running_experiments = set()
        
        for log_file in log_files:
            if self.is_experiment_running(log_file):
                running_exp = self.extract_running_experiment(log_file)
                if running_exp:
                    running_experiments.add(running_exp)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        status['completed'] = len(completed_experiments)
        status['running'] = len(running_experiments)
        status['pending'] = len(self.experiment_list) - status['completed'] - status['running']
        status['pipeline_completed'] = len(completed_experiments)
        
        if running_experiments:
            status['current_experiment'] = list(running_experiments)[0]
        
        # ìµœê·¼ ì™„ë£Œëœ ì‹¤í—˜ ì •ë³´
        status['recent_completed'] = self.get_recent_completed_experiments(completed_experiments)
        
        return status
    
    def extract_experiment_name(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ì‹¤í—˜ ì´ë¦„ ì¶”ì¶œ"""
        if filename.startswith("v3_") and filename.endswith(".csv"):
            # íŒŒì¼ëª…ì—ì„œ timestamp ì œê±°í•˜ì—¬ ì‹¤í—˜ ì´ë¦„ ì¶”ì¶œ
            name_parts = filename.replace(".csv", "").split("-")
            if len(name_parts) > 1:
                return name_parts[1]  # timestamp ë‹¤ìŒ ë¶€ë¶„ì´ ì‹¤í—˜ ì´ë¦„
        return None
    
    def is_experiment_running(self, log_file):
        """ì‹¤í—˜ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            if not lines:
                return False
            
            # ìµœê·¼ ë¡œê·¸ í™•ì¸
            recent_lines = lines[-50:]  # ìµœê·¼ 50ì¤„ í™•ì¸
            
            # ì‹¤í–‰ ì¤‘ í‘œì‹œ ì°¾ê¸°
            for line in recent_lines:
                if "Starting V3 Hierarchical" in line or "Training for Model" in line:
                    return True
                if "completed" in line or "failed" in line:
                    return False
            
            return False
        except:
            return False
    
    def extract_running_experiment(self, log_file):
        """ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜ ì´ë¦„ ì¶”ì¶œ"""
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # ìµœê·¼ ë¡œê·¸ì—ì„œ ì‹¤í—˜ ì´ë¦„ ì°¾ê¸°
            for line in reversed(lines[-20:]):
                if "Starting V3 Hierarchical:" in line:
                    return line.split(":")[-1].strip()
            
            return None
        except:
            return None
    
    def get_recent_completed_experiments(self, completed_experiments):
        """ìµœê·¼ ì™„ë£Œëœ ì‹¤í—˜ ì •ë³´ ìˆ˜ì§‘"""
        recent = []
        
        for exp_name in completed_experiments:
            # ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
            result_files = list(self.results_dir.glob(f"**/v3_*{exp_name}*.csv"))
            
            if result_files:
                result_file = result_files[0]
                exp_info = {
                    'name': exp_name,
                    'completed_time': datetime.fromtimestamp(result_file.stat().st_mtime),
                    'f1_score': self.extract_f1_score(result_file)
                }
                recent.append(exp_info)
        
        # ì™„ë£Œ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
        recent.sort(key=lambda x: x['completed_time'], reverse=True)
        return recent
    
    def extract_f1_score(self, result_file):
        """ê²°ê³¼ íŒŒì¼ì—ì„œ F1 ì ìˆ˜ ì¶”ì¶œ (ì¶”í›„ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” ë¡œê·¸ íŒŒì¼ì´ë‚˜ ë³„ë„ ê²°ê³¼ íŒŒì¼ì—ì„œ F1 ì ìˆ˜ë¥¼ ì¶”ì¶œí•´ì•¼ í•¨
        return "N/A"
    
    def display_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ í‘œì‹œ"""
        print("\nğŸ” V3 Hierarchical Performance Summary:")
        print("-" * 50)
        
        # ì™„ë£Œëœ ì‹¤í—˜ë“¤ì˜ ì„±ëŠ¥ ë¶„ì„
        completed_results = self.collect_v3_results()
        
        if not completed_results:
            print("ğŸ“Š No completed experiments found")
            return
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ í†µê³„
        model_a_performance = {}
        model_b_performance = {}
        
        for result in completed_results:
            model_a_name = result.get('model_a_name', 'unknown')
            model_b_name = result.get('model_b_name', 'unknown')
            
            if model_a_name not in model_a_performance:
                model_a_performance[model_a_name] = []
            if model_b_name not in model_b_performance:
                model_b_performance[model_b_name] = []
            
            model_a_performance[model_a_name].append(result.get('model_a_f1', 0))
            model_b_performance[model_b_name].append(result.get('model_b_f1', 0))
        
        # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜
        best_result = max(completed_results, key=lambda x: x.get('final_f1', 0))
        print(f"ğŸ† Best Performance: {best_result.get('name', 'unknown')}")
        print(f"   Final F1: {best_result.get('final_f1', 0):.4f}")
        print(f"   Model A F1: {best_result.get('model_a_f1', 0):.4f}")
        print(f"   Model B F1: {best_result.get('model_b_f1', 0):.4f}")
        
        # í‰ê·  ì„±ëŠ¥
        avg_f1 = np.mean([r.get('final_f1', 0) for r in completed_results])
        print(f"ğŸ“Š Average F1: {avg_f1:.4f}")
    
    def collect_v3_results(self):
        """V3 ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘"""
        results = []
        
        # ì œì¶œ íŒŒì¼ë“¤ ìŠ¤ìº”
        submission_files = list(self.results_dir.glob("**/v3_*.csv"))
        
        for file_path in submission_files:
            exp_name = self.extract_experiment_name(file_path.name)
            if exp_name:
                result = {
                    'name': exp_name,
                    'file_path': str(file_path),
                    'submission_time': datetime.fromtimestamp(file_path.stat().st_mtime),
                    # ì¶”í›„ ë¡œê·¸ íŒŒì¼ì—ì„œ ì¶”ì¶œí•  ì„±ëŠ¥ ì§€í‘œë“¤
                    'model_a_f1': 0.0,
                    'model_b_f1': 0.0,
                    'final_f1': 0.0,
                    'model_a_name': self.extract_model_name(exp_name, 'A'),
                    'model_b_name': self.extract_model_name(exp_name, 'B')
                }
                results.append(result)
        
        return results
    
    def extract_model_name(self, exp_name, model_type):
        """ì‹¤í—˜ ì´ë¦„ì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ"""
        parts = exp_name.split('_')
        if len(parts) >= 3:
            if model_type == 'A':
                return parts[1]  # ë‘ ë²ˆì§¸ ë¶€ë¶„ì´ model A
            elif model_type == 'B':
                return parts[2]  # ì„¸ ë²ˆì§¸ ë¶€ë¶„ì´ model B
        return 'unknown'
    
    def analyze_hierarchical_performance(self):
        """ê³„ì¸µì  ë¶„ë¥˜ ì„±ëŠ¥ ë¶„ì„"""
        print("ğŸ” V3 Hierarchical Classification Performance Analysis")
        print("=" * 60)
        
        results = self.collect_v3_results()
        
        if not results:
            print("ğŸ“Š No completed experiments found")
            return
        
        # ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_performance_report(results)
        
        # ì‹œê°í™” ìƒì„±
        self.create_performance_visualizations(results)
    
    def generate_performance_report(self, results):
        """ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ“Š Generating performance analysis report...")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„
        model_performance = {}
        
        for result in results:
            model_combo = f"{result['model_a_name']}_+_{result['model_b_name']}"
            if model_combo not in model_performance:
                model_performance[model_combo] = []
            model_performance[model_combo].append(result['final_f1'])
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = f"""# V3 Hierarchical Classification Performance Report
Generated at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ“Š Overall Statistics
- **Total Completed Experiments**: {len(results)}
- **Best Performance**: {max(results, key=lambda x: x['final_f1'])['final_f1']:.4f}
- **Average Performance**: {np.mean([r['final_f1'] for r in results]):.4f}
- **Performance Std**: {np.std([r['final_f1'] for r in results]):.4f}

## ğŸ† Top 10 Experiments
"""
        
        # ìƒìœ„ 10ê°œ ì‹¤í—˜
        top_results = sorted(results, key=lambda x: x['final_f1'], reverse=True)[:10]
        for i, result in enumerate(top_results, 1):
            report += f"{i}. **{result['name']}** - F1: {result['final_f1']:.4f}\n"
        
        report += "\n## ğŸ” Model Combination Analysis\n"
        for combo, scores in model_performance.items():
            avg_score = np.mean(scores)
            report += f"- **{combo}**: {avg_score:.4f} (Â±{np.std(scores):.4f})\n"
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = self.analysis_dir / f"v3_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ Performance report saved: {report_path}")
    
    def create_performance_visualizations(self, results):
        """ì„±ëŠ¥ ì‹œê°í™” ìƒì„±"""
        print("ğŸ“Š Creating performance visualizations...")
        
        if not results:
            return
        
        # ì„±ëŠ¥ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        plt.figure(figsize=(12, 8))
        
        # 1. ì „ì²´ ì„±ëŠ¥ ë¶„í¬
        plt.subplot(2, 2, 1)
        f1_scores = [r['final_f1'] for r in results]
        plt.hist(f1_scores, bins=20, alpha=0.7, color='skyblue')
        plt.title('V3 Hierarchical F1 Score Distribution')
        plt.xlabel('F1 Score')
        plt.ylabel('Frequency')
        
        # 2. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        plt.subplot(2, 2, 2)
        model_a_names = list(set(r['model_a_name'] for r in results))
        model_a_scores = {name: [] for name in model_a_names}
        
        for result in results:
            model_a_scores[result['model_a_name']].append(result['final_f1'])
        
        plt.boxplot([scores for scores in model_a_scores.values()], 
                   labels=model_a_names)
        plt.title('Model A Performance Comparison')
        plt.ylabel('F1 Score')
        plt.xticks(rotation=45)
        
        # 3. ì‹œê°„ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
        plt.subplot(2, 2, 3)
        sorted_results = sorted(results, key=lambda x: x['submission_time'])
        times = [r['submission_time'] for r in sorted_results]
        scores = [r['final_f1'] for r in sorted_results]
        
        plt.plot(times, scores, 'o-', alpha=0.7)
        plt.title('Performance Over Time')
        plt.xlabel('Submission Time')
        plt.ylabel('F1 Score')
        plt.xticks(rotation=45)
        
        # 4. ëª¨ë¸ ì¡°í•©ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ
        plt.subplot(2, 2, 4)
        model_a_names = sorted(set(r['model_a_name'] for r in results))
        model_b_names = sorted(set(r['model_b_name'] for r in results))
        
        heatmap_data = np.zeros((len(model_a_names), len(model_b_names)))
        
        for i, a_name in enumerate(model_a_names):
            for j, b_name in enumerate(model_b_names):
                combo_results = [r for r in results 
                               if r['model_a_name'] == a_name and r['model_b_name'] == b_name]
                if combo_results:
                    heatmap_data[i][j] = np.mean([r['final_f1'] for r in combo_results])
        
        sns.heatmap(heatmap_data, 
                   xticklabels=model_b_names, 
                   yticklabels=model_a_names,
                   annot=True, fmt='.3f', cmap='viridis')
        plt.title('Model Combination Performance Heatmap')
        plt.xlabel('Model B')
        plt.ylabel('Model A')
        
        plt.tight_layout()
        
        # ê·¸ë˜í”„ ì €ì¥
        viz_path = self.analysis_dir / f"v3_performance_viz_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(viz_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š Visualizations saved: {viz_path}")
    
    def export_results_csv(self):
        """ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        print("ğŸ“Š Exporting results to CSV...")
        
        results = self.collect_v3_results()
        
        if not results:
            print("ğŸ“Š No results to export")
            return
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(results)
        
        # CSV ì €ì¥
        csv_path = self.analysis_dir / f"v3_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(csv_path, index=False)
        
        print(f"ğŸ“„ Results exported to: {csv_path}")
        return csv_path

def main():
    parser = argparse.ArgumentParser(description="V3 Hierarchical Classification Experiment Monitor")
    parser.add_argument('--realtime', action='store_true', help='Real-time monitoring')
    parser.add_argument('--analyze', action='store_true', help='Analyze completed experiments')
    parser.add_argument('--export', action='store_true', help='Export results to CSV')
    parser.add_argument('--interval', type=int, default=60, help='Monitoring interval in seconds')
    
    args = parser.parse_args()
    
    try:
        monitor = V3ExperimentMonitor()
        
        if args.realtime:
            monitor.monitor_realtime(args.interval)
        elif args.analyze:
            monitor.analyze_hierarchical_performance()
        elif args.export:
            monitor.export_results_csv()
        else:
            # ê¸°ë³¸: í˜„ì¬ ìƒíƒœ í‘œì‹œ
            monitor.display_v3_status()
            
            print("\nğŸ”§ Available commands:")
            print("  --realtime    : Real-time monitoring")
            print("  --analyze     : Performance analysis")
            print("  --export      : Export results to CSV")
    
    except Exception as e:
        print(f"âŒ Error: {e}")
        raise

if __name__ == "__main__":
    main()
